{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f972f82d",
   "metadata": {},
   "source": [
    "### Basic working of Google Palm LLM in LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a34aa70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import GooglePalm\n",
    "\n",
    "api_key = 'AIzaSyDz2Dt2LonIglhqMihyIJiAFeIgT82p9XA'  # Get your API key from https://makersuite.google.com/\n",
    "\n",
    "llm = GooglePalm(google_api_key=api_key, temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4a514235",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import google.generativeai as palm\n",
    "\n",
    "palm.configure(api_key='AIzaSyDz2Dt2LonIglhqMihyIJiAFeIgT82p9XA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d2c9ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/text-bison-001\n"
     ]
    }
   ],
   "source": [
    "models=[m for m in palm.list_models() if 'generateText' in m.supported_generation_methods]\n",
    "model=models[0].name\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e712f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed4c35bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crispy shell, savory heart,\n",
      "Samosa, my culinary art.\n",
      "Golden brown, a symphony of spice,\n",
      "A taste of heaven, a perfect slice.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "genai.configure(api_key=\"AIzaSyDz2Dt2LonIglhqMihyIJiAFeIgT82p9XA\")\n",
    "\n",
    "model = genai.GenerativeModel(\"gemini-pro\")\n",
    "response = model.generate_content(\"Write a 4-line poem of my love for samosa\")\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c235a80e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response:\n",
      "GenerateContentResponse(\n",
      "    done=True,\n",
      "    iterator=None,\n",
      "    result=protos.GenerateContentResponse({\n",
      "      \"candidates\": [\n",
      "        {\n",
      "          \"content\": {\n",
      "            \"parts\": [\n",
      "              {\n",
      "                \"text\": \"Dear [Customer Service Representative Name],\\n\\nI am writing to request a refund for an electronic item I purchased from your company on [date of purchase]. The order number is [order number].\\n\\nThe [item name] I received was defective upon arrival. I have attempted to troubleshoot the issue myself, but have been unable to resolve it. I have attached a copy of my troubleshooting log for your reference.\\n\\nI am very disappointed with this product, as I was really looking forward to using it. I would appreciate it if you could process a full refund for my purchase.\\n\\nI am available to [return the item or provide proof of the defect] at your convenience. Please let me know the best way to proceed.\\n\\nThank you for your attention to this matter.\\n\\nSincerely,\\n[Your Name]\"\n",
      "              }\n",
      "            ],\n",
      "            \"role\": \"model\"\n",
      "          },\n",
      "          \"finish_reason\": \"STOP\",\n",
      "          \"index\": 0,\n",
      "          \"safety_ratings\": [\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
      "              \"probability\": \"NEGLIGIBLE\"\n",
      "            },\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
      "              \"probability\": \"NEGLIGIBLE\"\n",
      "            },\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
      "              \"probability\": \"NEGLIGIBLE\"\n",
      "            },\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
      "              \"probability\": \"NEGLIGIBLE\"\n",
      "            }\n",
      "          ]\n",
      "        }\n",
      "      ],\n",
      "      \"usage_metadata\": {\n",
      "        \"prompt_token_count\": 8,\n",
      "        \"candidates_token_count\": 163,\n",
      "        \"total_token_count\": 171\n",
      "      },\n",
      "      \"model_version\": \"gemini-pro\"\n",
      "    }),\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "essay = model.generate_content(\"write email requesting refund for electronic item\")\n",
    "print(essay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "227816a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import GooglePalmEmbeddings\n",
    "from langchain.llms import GooglePalm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765695b5",
   "metadata": {},
   "source": [
    "### Now let's load data from Codebasics FAQ csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9ed49a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'I have never done programming in my life. Can I take this bootcamp?', 'row': 0}, page_content='prompt: I have never done programming in my life. Can I take this bootcamp?\\nresponse: Yes, this is the perfect bootcamp for anyone who has never done coding and wants to build a career in the IT/Data Analytics industry or just wants to perform better in your current job or business using data.'), Document(metadata={'source': 'Why should I trust Codebasics?', 'row': 1}, page_content='prompt: Why should I trust Codebasics?\\nresponse: Till now 9000 + learners have benefitted from the quality of our courses. You can check the review section and also we have attached their LinkedIn profiles so that you can connect with them and ask directly.'), Document(metadata={'source': 'Is there any prerequisite for taking this bootcamp ?', 'row': 2}, page_content='prompt: Is there any prerequisite for taking this bootcamp ?\\nresponse: Our bootcamp is specifically designed for beginners with no prior experience in this field. The only prerequisite is that you need to have a functional laptop with at least 4GB ram, an internet connection, and a thrill to learn data analysis.'), Document(metadata={'source': 'What datasets are used in this bootcamp? Is it some toy datasets or\\n something that mimics a real-world business problem?', 'row': 3}, page_content='prompt: What datasets are used in this bootcamp? Is it some toy datasets or\\n something that mimics a real-world business problem?\\nresponse: The datasets used in this bootcamp are crafted from scratch to mimic the real business world by consolidating our years of experience. The datasets not just contain more than a million rows but also cover multiple facets of the business.'), Document(metadata={'source': 'I\\x92m not sure if this bootcamp is good enough for me to invest some \\nmoney. What can I do?', 'row': 4}, page_content='prompt: I\\x92m not sure if this bootcamp is good enough for me to invest some \\nmoney. What can I do?\\nresponse: We got you covered. Go ahead and watch our youtube videos if you like them and want to learn further then this bootcamp is the perfect extension.'), Document(metadata={'source': 'How can I contact the instructors for any doubts/support?', 'row': 5}, page_content='prompt: How can I contact the instructors for any doubts/support?\\nresponse: We have created every lecture with a motive to explain everything in an easy-to-understand manner. While working on these lectures you could make mistakes in steps or have some doubts. You need to commit yourself to hold patience, make efforts & diagnose the errors yourself by googling in order to become truly job ready. For any questions, that Google cannot answer or if you hit a wall - we got you covered! You can join our active discord community. which is a dedicated platform to discuss & clear your doubts with fellow learners & mentors.'), Document(metadata={'source': 'What if I don\\x92t like this bootcamp?', 'row': 6}, page_content='prompt: What if I don\\x92t like this bootcamp?\\nresponse: As promised we will give you a 100% refund based on the guidelines (Please refer to our course refund policy before enrolling).'), Document(metadata={'source': 'Does this bootcamp have lifetime access?', 'row': 7}, page_content='prompt: Does this bootcamp have lifetime access?\\nresponse: Yes'), Document(metadata={'source': 'What is the duration of this bootcamp? How long will it last?', 'row': 8}, page_content='prompt: What is the duration of this bootcamp? How long will it last?\\nresponse: You can complete all courses in 3 months if you dedicate 2-3 hours per day.'), Document(metadata={'source': 'Can I attend this bootcamp while working full time?', 'row': 9}, page_content='prompt: Can I attend this bootcamp while working full time?\\nresponse: Yes. This bootcamp is self-paced. You can learn on your own schedule.'), Document(metadata={'source': 'What kind of skills can I expect to learn in this bootcamp?', 'row': 10}, page_content='prompt: What kind of skills can I expect to learn in this bootcamp?\\nresponse: You can expect to learn tech skills like Python, Excel, SQL, and Power BI along with other skills like basic project management, stakeholder management, communication skills, and Business knowledge for domains like Finance, Supply chain, Sales, and Marketing.'), Document(metadata={'source': 'Do you provide any job assistance?', 'row': 11}, page_content='prompt: Do you provide any job assistance?\\nresponse: Yes, We help you with resume and interview preparation along with that we help you in building online credibility, and based on requirements we refer candidates to potential recruiters.'), Document(metadata={'source': 'Is this bootcamp enough for me in Microsoft Power BI and\\n Excel certifications?', 'row': 12}, page_content='prompt: Is this bootcamp enough for me in Microsoft Power BI and\\n Excel certifications?\\nresponse: Yes, this bootcamp will certainly help because we cover the majority of the skills measured in these exams. However, please be informed that this course focuses on Job ready aspects and not on all aspects required to clear the exams. In addition to this course, you might need to visit the official learning material designed by Microsoft which is available for free on their website.'), Document(metadata={'source': 'Do we have an EMI option?', 'row': 13}, page_content='prompt: Do we have an EMI option?\\nresponse: No'), Document(metadata={'source': 'Do you provide any virtual internship?', 'row': 14}, page_content='prompt: Do you provide any virtual internship?\\nresponse: Yes'), Document(metadata={'source': 'Will this bootcamp guarantee me a job?', 'row': 15}, page_content='prompt: Will this bootcamp guarantee me a job?\\nresponse: The courses included in this bootcamp are done by 9000+ learners and many of them have secured a job which gives us ample confidence that you will be able to get a job. However, we want to be honest and do not want to make any impractical promises! Our guarantee is to prepare you for the job market by teaching the most relevant skills, knowledge & timeless principles good enough to fetch the job.'), Document(metadata={'source': 'I have zero knowledge of Excel and belong to a non-technical\\n background. Can I take this course?', 'row': 16}, page_content='prompt: I have zero knowledge of Excel and belong to a non-technical\\n background. Can I take this course?\\nresponse: Yes, this is the perfect course for anyone who has never worked on excel and wants to build a career in the IT/Data Analytics industry or just wants to perform better in their current job or business using data.'), Document(metadata={'source': 'What are the things I need to know before starting this course?', 'row': 17}, page_content='prompt: What are the things I need to know before starting this course?\\nresponse: This course is for absolute beginners hence you do not need any specific skills other than basic familiarity with computers'), Document(metadata={'source': 'What is different in this course compared to hundreds of courses on the internet and free tutorials on YouTube?', 'row': 18}, page_content='prompt: What is different in this course compared to hundreds of courses on the internet and free tutorials on YouTube?\\nresponse: Most of the courses available on the internet teach you how to build x & y without any business context and do not prepare you for real business world problem-solving. This course is rather an experience in which you will learn Excel by solving real-life use cases in an imaginary company called AtliQ Hardware. The tutorials are very easy to understand and also have a lot of fun elements into them so that you don\\x92t get bored ??'), Document(metadata={'source': 'Can I add this course to my resume?', 'row': 19}, page_content='prompt: Can I add this course to my resume?\\nresponse: Yes. Absolutely you can mention the AtliQ Hardware project experience in your resume with the relevant skills that you will learn from this course'), Document(metadata={'source': 'I\\x92m not sure if this course is good enough for me to invest some money. What can I do?', 'row': 20}, page_content='prompt: I\\x92m not sure if this course is good enough for me to invest some money. What can I do?\\nresponse: Don\\x92t worry. Many videos in this course are free so watch them to get an idea of the quality of teaching. Dhaval Patel (the course instructor) runs a popular data science YouTube channel called Codebasics. On that, you can watch his videos and read comments to get an idea of his teaching style'), Document(metadata={'source': 'What dataset is used in this course? Is it some toy dataset or something that mimics a real-world problem?', 'row': 21}, page_content='prompt: What dataset is used in this course? Is it some toy dataset or something that mimics a real-world problem?\\nresponse: For initial basics, we have used a toy dataset of movies but for advanced concepts, we used a dataset that is crafted to mimic the real business world by consolidating our years of experience and the FMCG industry. That dataset contains more than 1 million records.'), Document(metadata={'source': 'Once purchased, is this course available for lifetime access?', 'row': 22}, page_content='prompt: Once purchased, is this course available for lifetime access?\\nresponse: Yes'), Document(metadata={'source': 'How can I get help if I have a doubt and need support?', 'row': 23}, page_content='prompt: How can I get help if I have a doubt and need support?\\nresponse: We have an active discord server where you can post your question and most of the time you will get an answer in a reasonable time frame.'), Document(metadata={'source': 'I have never done programming and belong to a non-technical background. Can I take this course?', 'row': 24}, page_content='prompt: I have never done programming and belong to a non-technical background. Can I take this course?\\nresponse: Yes, this is the perfect course for anyone who has never done coding and wants to build a career in the IT/Data Analytics industry or just wants to perform better in their current job or business using data.'), Document(metadata={'source': '\\nI don\\x92t have a laptop, can I take this course?', 'row': 25}, page_content='prompt: I don\\x92t have a laptop, can I take this course?\\nresponse: We recommend learning by doing and therefore you need to have a laptop or a PC (at least 4 GB ram).'), Document(metadata={'source': 'Will the course help me in PL - 300 Microsoft exam preparation?', 'row': 26}, page_content='prompt: Will the course help me in PL - 300 Microsoft exam preparation?\\nresponse: Yes, this course will certainly help because we cover the majority of the skills measured in the PL - 300 exam in this course. However, please be informed that this course focuses on Job ready aspects and not on all aspects required to clear PL - 300 exam. In addition to this course, you might need to visit the official learning material designed by Microsoft which is available for free -> https://docs.microsoft.com/en-us/certifications/exams/pl-300?tab=tab-learning-paths'), Document(metadata={'source': 'Will the course be upgraded when there are new features in Power BI?', 'row': 27}, page_content='prompt: Will the course be upgraded when there are new features in Power BI?\\nresponse: Yes, the course will be upgraded periodically based on the new features in Power BI, and learners who have already bought this course will have free access to the upgrades.'), Document(metadata={'source': 'I use tableau, can I take this course?', 'row': 28}, page_content='prompt: I use tableau, can I take this course?\\nresponse: Yes, you will still benefit from the concepts outside the tools that are discussed in this course such as business context, problem solving, project management tools, etc.'), Document(metadata={'source': '\\nPower BI or Tableau which one is better?', 'row': 29}, page_content='prompt: Power BI or Tableau which one is better?\\nresponse: This is a contextual question. If you are talking about a pure visualization tool Tableau is slightly better. Data connectors, modeling and transformation features are available in both. However, factually speaking Power BI is cheaper and offers tighter integration with the Microsoft environment. Since most companies use excel & Microsoft tools they start with Power BI or move towards Power BI for seamless integration with other Microsoft tools (called as Power platform). This makes the job openings grow at a much higher rate on Power BI and Power Platform. Also, Power BI has been leading the Gartner\\x92s magic quadrant in BI for the last few years as the industry leader.'), Document(metadata={'source': 'What dataset is used in this course? Is it some Toy dataset or something that mimics a real-world business problem?', 'row': 30}, page_content='prompt: What dataset is used in this course? Is it some Toy dataset or something that mimics a real-world business problem?\\nresponse: The dataset used in this course is crafted from scratch to mimic the real business world by consolidating our years of experience. The dataset not just contains more than a million rows but also covers multiple facets of business data such as sales, finance, targets, forecasts, products, etc.'), Document(metadata={'source': 'Does Power BI work in Mac OS/Ubuntu?', 'row': 31}, page_content='prompt: Does Power BI work in Mac OS/Ubuntu?\\nresponse: Power BI desktop works only in Windows OS. Please look into the system requirements section on this page. However, you can use a virtual machine to install and work with Power BI in other Operating systems.'), Document(metadata={'source': 'What business concepts and domains are covered in this course?', 'row': 32}, page_content='prompt: What business concepts and domains are covered in this course?\\nresponse: We have covered the core functions such as Sales, Marketing, Finance, and Supply Chain with their fundamentals related to this course. The domain you will learn in this course is consumer goods which is projected to have more openings and high data analytics requirements at least until 2030.'), Document(metadata={'source': 'Will this course guarantee me a job?', 'row': 33}, page_content='prompt: Will this course guarantee me a job?\\nresponse: We created a much lighter version of this course on YouTube available for free (click this link) and many people gave us feedback that they were able to fetch jobs (see testimonials). Now this paid course is at least 5x better than the YouTube course which gives us ample confidence that you will be able to get a job. However, we want to be honest and do not want to make any impractical promises! Our guarantee is to prepare you for the job market by teaching the most relevant skills, knowledge & timeless principles good enough to fetch the job.'), Document(metadata={'source': 'Can I add this course to my resume? If Yes, how?', 'row': 34}, page_content='prompt: Can I add this course to my resume? If Yes, how?\\nresponse: Absolutely, we have a section in this course explaining how you can add the learnings from this course in your resume that will appeal to the hiring team.'), Document(metadata={'source': 'Is there any prerequisite for taking this course?', 'row': 35}, page_content='prompt: Is there any prerequisite for taking this course?\\nresponse: The only prerequisite is that you need to have a functional laptop with at least 4GB ram, internet connection and a thrill to learn data analysis.'), Document(metadata={'source': 'What is different in this course from thousands of other Power BI courses available online?', 'row': 36}, page_content='prompt: What is different in this course from thousands of other Power BI courses available online?\\nresponse: Most of the courses available on the internet teach you how to build x & y without any business context and do not prepare you for the real business world. This course is rather an experience in which you will learn how to use Power BI & other non-technical skills to solve a real-life business problem using analytics. Here you focus on solving a business problem and in that process learn how Power BI can be used as a tool. This is how you will do the work when you start working as a data analyst/ Business analyst/ Power BI developer in the industry. This course will prepare you for not just fetching the job but, shine in it & grow further.'), Document(metadata={'source': 'I already know basic Power BI, what benefit do I get by taking this course?', 'row': 37}, page_content='prompt: I already know basic Power BI, what benefit do I get by taking this course?\\nresponse: This course is taught through a true end-to-end project in a Consumer goods company involving all the steps mimicking the real business environment, so you will learn how to execute end-to-end projects Power BI projects successfully along with the business fundamentals. You will learn a lot of extra things such as Project management tools, effective communication techniques & organizational nuances.'), Document(metadata={'source': 'How to install power pivot if its not available in system?', 'row': 38}, page_content='prompt: How to install power pivot if its not available in system?\\nresponse: Follow this thread for instructions - https://support.microsoft.com/en-us/office/start-the-power-pivot-add-in-for-excel-a891a66d-36e3-43fc-81e8-fc4798f39ea8\\nIf it doesn\\'t show in the ribbon then go to \"insert\" tab. You will be able to see pivot table option there.'), Document(metadata={'source': 'Hi all, could someone please help me understand what I am writing wrong in this code?', 'row': 39}, page_content='prompt: Hi all, could someone please help me understand what I am writing wrong in this code?\\nresponse: Go to ChatGPT (https://chat.openai.com/) and type this command\\n\\nDebug me this code ( copy paste your code)\\n\\n\\nWhy I am suggesting this way is once you learn it you will love it. \\nAnd it will be SUPER USEFUL in your career when you get a data analyst jobs. Companies love folks who are capable of finding their answers on their own (Using Google, ChatGPT etc.)'), Document(metadata={'source': 'I am getting this error, what is wrong?', 'row': 40}, page_content='prompt: I am getting this error, what is wrong?\\nresponse: Go to ChatGPT (https://chat.openai.com/) and type this command\\n\\nDebug me this code ( copy paste your code) and also provide the error you are getting\\n\\n\\nWhy I am suggesting this way is once you learn it you will love it. \\nAnd it will be SUPER USEFUL in your career when you get a data analyst jobs. Companies love folks who are capable of finding their answers on their own (Using Google, ChatGPT etc.)'), Document(metadata={'source': \"Hey all, I've been trying this for past one hour but not working. \\nI am getting this error in my SQL code, what could be wrong?\", 'row': 41}, page_content=\"prompt: Hey all, I've been trying this for past one hour but not working. \\nI am getting this error in my SQL code, what could be wrong?\\nresponse: Go to ChatGPT (https://chat.openai.com/) and type this command\\n\\nDebug me this code ( copy paste your code) and also provide the error you are getting\\n\\n\\nWhy I am suggesting this way is once you learn it you will love it. \\nAnd it will be SUPER USEFUL in your career when you get a data analyst jobs. Companies love folks who are capable of finding their answers on their own (Using Google, ChatGPT etc.)\"), Document(metadata={'source': 'when I am entering the index match formula in the cell outside the table it is working and when i enter the same formula inside the table cell it is showing a SPILL! error ..why?', 'row': 42}, page_content='prompt: when I am entering the index match formula in the cell outside the table it is working and when i enter the same formula inside the table cell it is showing a SPILL! error ..why?\\nresponse: Go to ChatGPT (https://chat.openai.com/) and ask this same question.\\n\\nWhy I am suggesting this way is once you learn it you will love it. \\nAnd it will be SUPER USEFUL in your career when you get a data analyst jobs. Companies love folks who are capable of finding their answers on their own (Using Google, ChatGPT etc.)'), Document(metadata={'source': 'I am not allowing to post doubt in the discord group', 'row': 43}, page_content=\"prompt: I am not allowing to post doubt in the discord group\\nresponse: Sure I can guide you\\n\\nGo to the 'click-here-to-ask-questions' section and verify yourself here by clicking on the checkmark.\"), Document(metadata={'source': 'How can I use PowerBI on my Mac system?', 'row': 44}, page_content='prompt: How can I use PowerBI on my Mac system?\\nresponse: Hi\\n\\nYou can use VirtualBox to create a virtual machine and install Windows on it. This will allow you to run Power BI and Excel on your Mac.\\n\\nIf you\\'re not familiar with setting up a virtual machine, there are many resources available on YouTube that can guide you through the process. Simply search for \"installing virtual machines\" and you\\'ll find plenty of helpful videos.\\n\\nBest of luck with your studies!'), Document(metadata={'source': 'In sales analytics chapter , when creating pivot table a blank column is getting added. How to solve this?', 'row': 45}, page_content='prompt: In sales analytics chapter , when creating pivot table a blank column is getting added. How to solve this?\\nresponse: Hi, please check these instructions.\\n\\nOption - 1:\\nIf you are from India, you can change your system default date format to India, then restart your computer once. and see, whether it fixed your issue or not. Else you can follow the second option.\\n\\nOption - 2:\\n\\n1. Reason for the Issue:\\nIn the dim_date table, the date is in dd/mm/yyyy format. Whereas the fact table is in mm/dd/yyyy format. Because of the different formats, causing the issue. Power BI/Excel will likely use your system\\'s default date format whenever you open any file. This could cause the date format issue - dd/mm/yyyy --- mm/dd/yyyy. Please look for this whenever you face issues with values mismatch.\\n\\n2. Steps to resolve:\\n    1. Change the data type of the date column to \\x91Date\\x92 if it is in \\x91Date/Time\\x92 format. Skip this step if it is already in \\x91Date\\x92 format.\\n    2. Create a new custom column in the fact_sales_monthly table. you can name it date_modified or any other name accordingly.\\n    Formula\\n    = Date.ToText([date],\"yyyy\") & \"/\" & Text.PadStart(Text.From(Date.Day([date])),2,\"0\") & \"/\" & Text.PadStart(Text.From(Date.Month([date])),2,\"0\")\\n    3. change the data type of the above-created column to date.\\n    4. In Power Pivot, Go to the \\'Manage\\' tab and select the \\'Diagram\\' view. Map the date column in the \\'dim_date\\' table with the newly created column in the \\'fact_sales_monthly\\' table. Make sure to delete the previous relationship (date from fact_sales_month).'), Document(metadata={'source': 'why row and value option is not showing for the visual in PowerBI , any setting need to be change, please let me know?', 'row': 46}, page_content='prompt: why row and value option is not showing for the visual in PowerBI , any setting need to be change, please let me know?\\nresponse: You have selected Table Visual instead of Matrix. That is why you are seeing a different interface.'), Document(metadata={'source': 'Hello, I have a issue to calculate WHF%. Can you give me numbers for checklist and debbuging?', 'row': 47}, page_content='prompt: Hello, I have a issue to calculate WHF%. Can you give me numbers for checklist and debbuging?\\nresponse: Hello \\n\\nPlease confirm whether you got these values right!\\n\\nBefore Data Cleaning Stats:\\nNumber of rows: 6336\\nNumber of distinct employees: 198\\nNumber of distinct employee_id: 106\\n\\nAfter Data Cleaning Stats:\\nNumber of rows: 6208\\nNumber of distinct employees: 99\\nNumber of distinct employee_id: 74\\n\\nIf these values are not matching, I highly suggest doing the data-cleaning steps correctly (mainly removing duplicates)'), Document(metadata={'source': 'How do become good and comfortable with SQL?', 'row': 48}, page_content=\"prompt: How do become good and comfortable with SQL?\\nresponse: To master SQL, we need to practice it every day and keep in touch with it all the time.\\n\\n---\\nHere are some references where you can practice daily. First start with easy problems, then medium, and finally hard questions. Don't directly jump into Hard questions.\\n- Hacker rank: https://www.hackerrank.com/domains/sql\\n- Leetcode: https://leetcode.com/problemset/database/\\n- Datalemur: https://datalemur.com/\\n---\\n\\nI have created a LinkedIn post attaching some youtube resources which discussed interview-related problems and help to solve interesting questions. check out this too.\\nLink:  https://www.linkedin.com/posts/kirandeepmarala_sql-careers-data-activity-7012258071334854656-jRgf?utm_source=share&utm_medium=member_desktop\\n---\\n\\nAlso, I highly recommend participating in the Codebasics SQL challenge, which has been conducted in the past, as it can help you brush up on your skills and provide an opportunity to test your knowledge further\\nLink:  https://codebasics.io/challenge/codebasics-resume-project-challenge  (Challenge #4)\"), Document(metadata={'source': 'Why SUM() function not working in my Excel?', 'row': 49}, page_content='prompt: Why SUM() function not working in my Excel?\\nresponse: Because of having  #N/A values, the SUM() function is not working as expected.\\n\\nso we need to handle this case by filling the #N/A(not available) with 0.\\n\\nThread for further explanation: https://discord.com/channels/1090613684163850280/1107909641989525524/1107926537988227073'), Document(metadata={'source': 'What is the difference between Business Analyst Vs Data Analyst', 'row': 50}, page_content='prompt: What is the difference between Business Analyst Vs Data Analyst\\nresponse: While both business analysts and data analysts work with data, they have different focuses and responsibilities. Here are some key distinctions:\\n\\n1. Focus: Business analysts focus on the business side of things, such as business processes, workflows, and strategies. They use data analysis to identify business problems, opportunities, and solutions. Data analysts, on the other hand, focus on the data itself. They collect, process, and analyze data to extract insights and make data-driven decisions.\\n2. Skill set: Business analysts need to have strong communication, problem-solving, and strategic thinking skills, as they often work closely with stakeholders and decision-makers to identify business needs and develop solutions. They also need to have some knowledge of data analysis, such as data visualization and basic statistical analysis. Data analysts, on the other hand, need to have a strong foundation in data analysis and programming skills, such as SQL, Python, etc. They also need to have a good understanding of data visualization and statistical analysis to derive insights from the data.\\n3. Tools: Business analysts typically use tools like spreadsheets, business process modeling software, and business intelligence (BI) tools to analyze data and create reports. Data analysts use more advanced tools like statistical software, programming languages, and database management systems to collect, process, and analyze data.\\n4. Outputs: Business analysts typically produce reports, presentations, and recommendations based on their analysis of the data. Data analysts produce statistical models, visualizations, and reports to help stakeholders understand the data and make informed decisions.\\n\\nI recommend watching this video by Dhaval sir which explained more details: https://www.youtube.com/watch?v=7sFRpbPKTG4&t=1s'), Document(metadata={'source': 'Can you state the distinction among the SAMEPERIODLASTYEAR(), PARALLELPERIOD(), and DATEADD() function in Power BI?', 'row': 51}, page_content=\"prompt: Can you state the distinction among the SAMEPERIODLASTYEAR(), PARALLELPERIOD(), and DATEADD() function in Power BI?\\nresponse: Here is the brief explanation.\\n\\nSAMEPERIODLASTYEAR() compares the given period with the exact same period of last year as the name suggests. But it has a limitation like King of the chess board - it can go only compare 1 year backward for a given day, month, week, or quarter. If you want to go backward or forward or if you need to go 3 or 6 months/years/days you need to use DATEADD().\\n\\nDATEADD() is more like the queen of the chessboard. It is very dynamic as it compares a day, month, quarter, year, etc. over any specified no. of time period.\\n\\nPARALLELPERIOD() works exactly like DATEADD() but there is a minor difference. Parallelperiod() does not work dynamically like DATEADD().\\n\\nFor example, if you use PARALLELPERIOD() to get the previous month's value for dates between 1/5/2023 - 12/5/2023..it will compare these 12 days with the entire previous month of April 2023 (1/4/2023 - 30/4/2023) whereas DATEADD will compare the exact same days (1/4/2023- 12/4/2023).\\n\\nLikewise, if you use PARALLELPERIOD() to get the previous year's value for dates between 1/1/2023 - 12/5/2023..it will compare this time period with the entire previous year 2022 (1/1/2022 - 31/12/2022) whereas DATEADD will compare the exact same period (1/1/2022- 12/5/2022).\\n\\nRefer this thread for more info: https://discord.com/channels/1090613684163850280/1106528524023648326\"), Document(metadata={'source': 'My Excel Excel VLookup() showing formula instead of result', 'row': 52}, page_content=\"prompt: My Excel Excel VLookup() showing formula instead of result\\nresponse: Solution1:\\n\\nPlease check the cell type, if it is 'text', change that to 'general', and once hit enter execute the formula.\\n\\nSolution2:\\n\\nThe primary reason for this issue is the accidental enabling of the 'show formulas' mode.\\n To gain more clarity and resolve the problem, please refer to the following resource: https://exceljet.net/articles/excel-shows-formula-not-result\"), Document(metadata={'source': 'I have a doubt regarding Vlookup() giving zero values in Exercise-3.', 'row': 53}, page_content='prompt: I have a doubt regarding Vlookup() giving zero values in Exercise-3.\\nresponse: check this demo video to fix the issue: https://drive.google.com/file/d/1gp0krl2wMN0YhZYja3IKmt59SBsUK7xB/view?usp=share_link'), Document(metadata={'source': 'i am unable to import data from mysql in power bi ,connector issue is coming continuously i have done all steps according to connector pdf still its not resolving please guide', 'row': 54}, page_content='prompt: i am unable to import data from mysql in power bi ,connector issue is coming continuously i have done all steps according to connector pdf still its not resolving please guide\\nresponse: Please refer to this thread: https://discord.com/channels/1090613684163850280/1107992760105054238/1107993007606730802'), Document(metadata={'source': 'Do data analysts need to learn database design?', 'row': 55}, page_content='prompt: Do data analysts need to learn database design?\\nresponse: As a data analyst, it is not typically expected that you would be responsible for creating a database from scratch. However, you may be involved in designing the structure and schema of a database, as well as configuring and populating it with data.\\n\\nCreating a database usually falls under the purview of a database administrator (DBA) or a data engineer. These roles are focused on the technical aspects of database design, implementation, and maintenance.\\n\\nAs a data analyst, your primary responsibilities would typically include querying and analyzing data within an existing database, creating reports and visualizations to communicate insights, and helping to inform business decisions based on data analysis. However, depending on the organization and the specific role, there may be some overlap or variation in responsibilities.'), Document(metadata={'source': 'Can you explain me more about the foreign key?', 'row': 56}, page_content='prompt: Can you explain me more about the foreign key?\\nresponse: A foreign key is a column or set of columns in a table that refers to the primary key of another table. It is used to establish a relationship between two tables in a relational database. The primary key of one table is used as a foreign key in another table to link the data between the tables.\\n\\nFor example, let\\'s say we have two tables in a database: a \"customers\" table and an \"orders\" table. The customer\\'s table has a primary key column called \"customer_id\", and the orders table has a foreign key column called \"customer_id\" that references the \"customer_id\" column in the customer\\'s table.\\n\\nBy establishing this relationship between the two tables, we can ensure that every order in the orders table is associated with a valid customer in the customer\\'s table.'), Document(metadata={'source': \"I'm experiencing an issue where the target value appears the same in every row, and it is incorrect. Can you please help me understand why this is happening?\", 'row': 57}, page_content='prompt: I\\'m experiencing an issue where the target value appears the same in every row, and it is incorrect. Can you please help me understand why this is happening?\\nresponse: That is happening because you are pulling \"market\" from wrong table\\nPull \"Market\" From dim_market\\n\\nCheck out this thread : https://discord.com/channels/1090613684163850280/1108671672434839562/1108672494958817301'), Document(metadata={'source': 'Hello Everyone, I am using 2016 excel. So far, I have noticed there are few functions which are not available in 2016 Excel like xlookup, unique etc. is there any option \\navailable other than buying new updated version ?', 'row': 58}, page_content='prompt: Hello Everyone, I am using 2016 excel. So far, I have noticed there are few functions which are not available in 2016 Excel like xlookup, unique etc. is there any option \\navailable other than buying new updated version ?\\nresponse: Here are some add-in links. Please go through the videos , you will be able to use those functions in your Excel version. \\n\\n? UNIQUE function:\\n?? https://youtu.be/wiCH-YTacKU\\n\\n?XLOOKUP function: \\n?? https://www.youtube.com/watch?v=NWZaM7ZbE3Q\\nor,\\n?? https://www.youtube.com/watch?v=e60-J_u-K7o\\n\\n?? https://www.youtube.com/watch?v=TUTMfhCS62M'), Document(metadata={'source': 'Can someone help me to create same chart exactly like this. I have created more than half of it. I want to add quadrant labels. Low medium high on x & y axis', 'row': 59}, page_content='prompt: Can someone help me to create same chart exactly like this. I have created more than half of it. I want to add quadrant labels. Low medium high on x & y axis\\nresponse: Check this thread for instructions : https://discord.com/channels/1090613684163850280/1107745807848980490/1107746891418058762'), Document(metadata={'source': 'whenever I am loading fact_sales_monthly table in Power query not all columns are visible', 'row': 60}, page_content=\"prompt: whenever I am loading fact_sales_monthly table in Power query not all columns are visible\\nresponse: Have you taken SQL course in Codebasics first ?\\nIf yes, delete the existing databases in MySQL workbench and re-import again. After this go to 'Home' tab in Power Query and click on Refresh button and see if it works any.\\n\\nCheck this reference too: https://discordapp.com/channels/1090613684163850280/1111180401478746163/1111190746507251732\"), Document(metadata={'source': 'The fact_sales_monthly table seems to be missing. Could you please provide information about why it is not available?', 'row': 61}, page_content=\"prompt: The fact_sales_monthly table seems to be missing. Could you please provide information about why it is not available?\\nresponse: Delete the existing databases in MySQL workbench and re-import again. After this go to 'Home' tab in Power Query and click on Refresh button and see if it works any.\"), Document(metadata={'source': 'I am getting this error in PowerBI, \\n\\n\"there are pending changes in your queries that have havent been applied\"\\n\\nAny suggestions to solve it?', 'row': 62}, page_content='prompt: I am getting this error in PowerBI, \\n\\n\"there are pending changes in your queries that have havent been applied\"\\n\\nAny suggestions to solve it?\\nresponse: To resolve the annoying message, follow these steps:\\n1. Access Power Query by selecting \\'Transform Data.\\'\\n2. Click on \\'Refresh Preview\\' and then choose \\'Refresh All\\' to run a refresh on the tables.\\n3. After completing the refresh, select \\'Close & Apply\\' to save the changes.\\nBy following these steps, the annoying message should disappear.\\n\\nCheck this reference for further details: https://community.fabric.microsoft.com/t5/Desktop/quot-There-are-pending-changes-in-your-queries-that-haven-t-been/td-p/1142424\\nHope this helps!'), Document(metadata={'source': 'I am encountering an error with the pre-invoice discount where I am receiving \"0\" values. Could you assist me in resolving this issue?', 'row': 63}, page_content='prompt: I am encountering an error with the pre-invoice discount where I am receiving \"0\" values. Could you assist me in resolving this issue?\\nresponse: To troubleshoot the issue, please consider the following steps:\\n\\n1. Verify your data modeling by accessing \"Manage Relationships.\"\\n2. Review the measures you have created for any potential errors or inconsistencies.\\n3. Ensure that the column format is set correctly as the \"Fixed Decimal Number\" type.\\nBy examining these aspects, you can address the pre-invoice discount error and resolve the issue.\\n\\nCheck this reference for further details: https://discordapp.com/channels/1090613684163850280/1106513101693669466/1106516289905766433'), Document(metadata={'source': 'The year column is missing in the P&L check. How can I resolve this issue and obtain the year column?', 'row': 64}, page_content='prompt: The year column is missing in the P&L check. How can I resolve this issue and obtain the year column?\\nresponse: Check this reference:\\n https://discord.com/channels/1090613684163850280/1111101322406658098/1111137901816848494'), Document(metadata={'source': 'There are missing values in the P&L check. How can I address this issue and handle the missing values appropriately?', 'row': 65}, page_content='prompt: There are missing values in the P&L check. How can I address this issue and handle the missing values appropriately?\\nresponse: Check this reference:\\nhttps://discordapp.com/channels/1090613684163850280/1108980495477395486/1109016973662240848'), Document(metadata={'source': 'How can I rearrange the column order in the P&L check? I would like to change the order of the columns.', 'row': 66}, page_content='prompt: How can I rearrange the column order in the P&L check? I would like to change the order of the columns.\\nresponse: Check this reference:\\nhttps://discord.com/channels/1090613684163850280/1112342820930453516/1112399292792062022'), Document(metadata={'source': 'Why is the year 2018 missing or disappeared?', 'row': 67}, page_content='prompt: Why is the year 2018 missing or disappeared?\\nresponse: Check this reference:\\nhttps://discordapp.com/channels/1090613684163850280/1111545547426369637/1111563527753318430'), Document(metadata={'source': \"I'm experiencing a flat line in the Line chart of the Finance View. How can I address this issue and ensure the chart displays the expected data correctly?\", 'row': 68}, page_content=\"prompt: I'm experiencing a flat line in the Line chart of the Finance View. How can I address this issue and ensure the chart displays the expected data correctly?\\nresponse: Check this reference:\\nhttps://discord.com/channels/1090613684163850280/1112243985784774687/1112244353126113302\"), Document(metadata={'source': 'During the process of creating the line chart for forecast accuracy vs month in the Supply Chain View, I encountered an issue with the visual representation.\\nThe problem occurred at the 5:00-minute mark. I selected the year 2021, expecting the graph to display data only from September 2020 to August 2021.\\nHowever, instead of the desired timeframe, the chart displayed data from all months between 2017 and 2022.\\n\\nHow to solve and fix this issue?\\n', 'row': 69}, page_content='prompt: During the process of creating the line chart for forecast accuracy vs month in the Supply Chain View, I encountered an issue with the visual representation.\\nThe problem occurred at the 5:00-minute mark. I selected the year 2021, expecting the graph to display data only from September 2020 to August 2021.\\nHowever, instead of the desired timeframe, the chart displayed data from all months between 2017 and 2022.\\n\\nHow to solve and fix this issue?\\nresponse: Check this reference:\\nhttps://discordapp.com/channels/1090613684163850280/1113690341929918505/1113750218618445824'), Document(metadata={'source': 'It appears that the X-axis of the chart is not starting with the month of September as expected. Instead, it is starting with a different month.\\nHow to solve this?', 'row': 70}, page_content='prompt: It appears that the X-axis of the chart is not starting with the month of September as expected. Instead, it is starting with a different month.\\nHow to solve this?\\nresponse: Check this reference:\\nhttps://discordapp.com/channels/1090613684163850280/1112567189724213339/1112596364862423090'), Document(metadata={'source': 'Why we use Net error in place of absolute net error', 'row': 71}, page_content='prompt: Why we use Net error in place of absolute net error\\nresponse: Directional Insight: The net error metric offers valuable information regarding the direction of the error in forecasting, distinguishing between positive (overestimation) and negative (underestimation) values. This enables the identification of specific products that consistently outperform or underperform relative to the expected values. Additionally, net error provides insights into the risk factor, indicating if there are instances of stockouts or excessive inventory levels.'), Document(metadata={'source': 'I am encountering an issue where the NIS IND column has the same value in all rows. As a result, I am unable to obtain the correct value for the benchmark variable.\\n How can I address this problem and obtain accurate benchmark values?', 'row': 72}, page_content=\"prompt: I am encountering an issue where the NIS IND column has the same value in all rows. As a result, I am unable to obtain the correct value for the benchmark variable.\\n How can I address this problem and obtain accurate benchmark values?\\nresponse: Have you taken the 'market' column from dim_market? Can you please check again. \\n\\nCheck this reference for further details: https://discordapp.com/channels/1090613684163850280/1107202294476443789/1107208944931311647\"), Document(metadata={'source': 'How do I update source in power query ?', 'row': 73}, page_content='prompt: How do I update source in power query ?\\nresponse: Follow the discord link : \\n\\n https://discord.com/channels/1090613684163850280/1114113976616353832/1114121330925768754'), Document(metadata={'source': 'How do I enable Power Pivot before using it for the first time ?', 'row': 74}, page_content='prompt: How do I enable Power Pivot before using it for the first time ?\\nresponse: Follow the process in the link : \\n\\nhttps://drive.google.com/file/d/1-mO-v52h-YTY1s-v30liBJPu6Yj4OUxb/view?usp=share_link')]\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "loader = CSVLoader(file_path=\"codebasics_faqs.csv\", source_column=\"prompt\", encoding=\"ISO-8859-1\")\n",
    "data = loader.load()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd45e51",
   "metadata": {},
   "source": [
    "### Hugging Face Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "17b42a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.6.0-cp312-cp312-win_amd64.whl.metadata (28 kB)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.48.1-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-3.4.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: langchain in c:\\users\\deeks\\anaconda3\\lib\\site-packages (0.0.284)\n",
      "Requirement already satisfied: filelock in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from torch) (75.8.0)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Collecting huggingface-hub<1.0,>=0.24.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.28.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.5.2-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: Pillow in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from langchain) (2.0.34)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from langchain) (3.10.5)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from langchain) (0.5.14)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.21 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from langchain) (0.0.92)\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from langchain) (2.8.7)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from langchain) (2.8.2)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.11.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.26.0)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from pydantic<3,>=1->langchain) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.12.14)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (1.0.0)\n",
      "Downloading torch-2.6.0-cp312-cp312-win_amd64.whl (204.1 MB)\n",
      "   ---------------------------------------- 0.0/204.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/204.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.5/204.1 MB 1.7 MB/s eta 0:02:02\n",
      "   ---------------------------------------- 0.8/204.1 MB 1.5 MB/s eta 0:02:14\n",
      "   ---------------------------------------- 1.0/204.1 MB 1.5 MB/s eta 0:02:18\n",
      "   ---------------------------------------- 1.3/204.1 MB 1.5 MB/s eta 0:02:20\n",
      "   ---------------------------------------- 1.6/204.1 MB 1.6 MB/s eta 0:02:11\n",
      "   ---------------------------------------- 2.1/204.1 MB 1.6 MB/s eta 0:02:10\n",
      "   ---------------------------------------- 2.4/204.1 MB 1.7 MB/s eta 0:02:02\n",
      "    --------------------------------------- 2.9/204.1 MB 1.7 MB/s eta 0:01:57\n",
      "    --------------------------------------- 3.7/204.1 MB 1.9 MB/s eta 0:01:48\n",
      "    --------------------------------------- 4.2/204.1 MB 2.0 MB/s eta 0:01:41\n",
      "    --------------------------------------- 4.7/204.1 MB 2.1 MB/s eta 0:01:37\n",
      "   - -------------------------------------- 5.5/204.1 MB 2.2 MB/s eta 0:01:32\n",
      "   - -------------------------------------- 6.3/204.1 MB 2.3 MB/s eta 0:01:27\n",
      "   - -------------------------------------- 6.8/204.1 MB 2.3 MB/s eta 0:01:25\n",
      "   - -------------------------------------- 7.6/204.1 MB 2.4 MB/s eta 0:01:22\n",
      "   - -------------------------------------- 8.4/204.1 MB 2.5 MB/s eta 0:01:19\n",
      "   - -------------------------------------- 8.9/204.1 MB 2.5 MB/s eta 0:01:17\n",
      "   - -------------------------------------- 9.7/204.1 MB 2.6 MB/s eta 0:01:15\n",
      "   -- ------------------------------------- 10.7/204.1 MB 2.7 MB/s eta 0:01:12\n",
      "   -- ------------------------------------- 11.5/204.1 MB 2.7 MB/s eta 0:01:11\n",
      "   -- ------------------------------------- 12.3/204.1 MB 2.8 MB/s eta 0:01:09\n",
      "   -- ------------------------------------- 12.8/204.1 MB 2.8 MB/s eta 0:01:09\n",
      "   -- ------------------------------------- 13.6/204.1 MB 2.8 MB/s eta 0:01:08\n",
      "   -- ------------------------------------- 14.4/204.1 MB 2.9 MB/s eta 0:01:07\n",
      "   -- ------------------------------------- 15.2/204.1 MB 2.9 MB/s eta 0:01:06\n",
      "   --- ------------------------------------ 15.5/204.1 MB 2.9 MB/s eta 0:01:07\n",
      "   --- ------------------------------------ 16.5/204.1 MB 2.9 MB/s eta 0:01:05\n",
      "   --- ------------------------------------ 17.0/204.1 MB 2.9 MB/s eta 0:01:05\n",
      "   --- ------------------------------------ 18.1/204.1 MB 3.0 MB/s eta 0:01:03\n",
      "   --- ------------------------------------ 18.9/204.1 MB 3.0 MB/s eta 0:01:03\n",
      "   --- ------------------------------------ 19.4/204.1 MB 3.0 MB/s eta 0:01:02\n",
      "   ---- ----------------------------------- 20.4/204.1 MB 3.0 MB/s eta 0:01:01\n",
      "   ---- ----------------------------------- 21.2/204.1 MB 3.1 MB/s eta 0:01:00\n",
      "   ---- ----------------------------------- 22.0/204.1 MB 3.1 MB/s eta 0:00:59\n",
      "   ---- ----------------------------------- 22.8/204.1 MB 3.1 MB/s eta 0:00:59\n",
      "   ---- ----------------------------------- 23.6/204.1 MB 3.1 MB/s eta 0:00:58\n",
      "   ---- ----------------------------------- 24.1/204.1 MB 3.1 MB/s eta 0:00:58\n",
      "   ---- ----------------------------------- 24.9/204.1 MB 3.1 MB/s eta 0:00:58\n",
      "   ----- ---------------------------------- 26.0/204.1 MB 3.2 MB/s eta 0:00:57\n",
      "   ----- ---------------------------------- 26.5/204.1 MB 3.2 MB/s eta 0:00:57\n",
      "   ----- ---------------------------------- 27.3/204.1 MB 3.2 MB/s eta 0:00:56\n",
      "   ----- ---------------------------------- 28.3/204.1 MB 3.2 MB/s eta 0:00:55\n",
      "   ----- ---------------------------------- 28.8/204.1 MB 3.2 MB/s eta 0:00:55\n",
      "   ----- ---------------------------------- 29.4/204.1 MB 3.2 MB/s eta 0:00:55\n",
      "   ----- ---------------------------------- 30.1/204.1 MB 3.2 MB/s eta 0:00:55\n",
      "   ------ --------------------------------- 30.7/204.1 MB 3.2 MB/s eta 0:00:55\n",
      "   ------ --------------------------------- 31.5/204.1 MB 3.2 MB/s eta 0:00:54\n",
      "   ------ --------------------------------- 32.2/204.1 MB 3.2 MB/s eta 0:00:54\n",
      "   ------ --------------------------------- 33.0/204.1 MB 3.2 MB/s eta 0:00:54\n",
      "   ------ --------------------------------- 33.8/204.1 MB 3.2 MB/s eta 0:00:53\n",
      "   ------ --------------------------------- 34.6/204.1 MB 3.2 MB/s eta 0:00:53\n",
      "   ------ --------------------------------- 35.4/204.1 MB 3.2 MB/s eta 0:00:53\n",
      "   ------- -------------------------------- 35.9/204.1 MB 3.2 MB/s eta 0:00:53\n",
      "   ------- -------------------------------- 36.7/204.1 MB 3.2 MB/s eta 0:00:52\n",
      "   ------- -------------------------------- 37.5/204.1 MB 3.2 MB/s eta 0:00:52\n",
      "   ------- -------------------------------- 38.3/204.1 MB 3.2 MB/s eta 0:00:52\n",
      "   ------- -------------------------------- 39.1/204.1 MB 3.3 MB/s eta 0:00:51\n",
      "   ------- -------------------------------- 39.6/204.1 MB 3.3 MB/s eta 0:00:51\n",
      "   ------- -------------------------------- 40.6/204.1 MB 3.3 MB/s eta 0:00:51\n",
      "   -------- ------------------------------- 41.4/204.1 MB 3.3 MB/s eta 0:00:50\n",
      "   -------- ------------------------------- 42.2/204.1 MB 3.3 MB/s eta 0:00:50\n",
      "   -------- ------------------------------- 43.0/204.1 MB 3.3 MB/s eta 0:00:49\n",
      "   -------- ------------------------------- 43.8/204.1 MB 3.3 MB/s eta 0:00:49\n",
      "   -------- ------------------------------- 44.6/204.1 MB 3.3 MB/s eta 0:00:49\n",
      "   -------- ------------------------------- 45.1/204.1 MB 3.3 MB/s eta 0:00:49\n",
      "   -------- ------------------------------- 45.9/204.1 MB 3.3 MB/s eta 0:00:48\n",
      "   --------- ------------------------------ 46.7/204.1 MB 3.3 MB/s eta 0:00:48\n",
      "   --------- ------------------------------ 47.4/204.1 MB 3.3 MB/s eta 0:00:47\n",
      "   --------- ------------------------------ 48.5/204.1 MB 3.3 MB/s eta 0:00:47\n",
      "   --------- ------------------------------ 49.0/204.1 MB 3.3 MB/s eta 0:00:47\n",
      "   --------- ------------------------------ 49.8/204.1 MB 3.3 MB/s eta 0:00:47\n",
      "   --------- ------------------------------ 50.6/204.1 MB 3.3 MB/s eta 0:00:46\n",
      "   ---------- ----------------------------- 51.4/204.1 MB 3.4 MB/s eta 0:00:46\n",
      "   ---------- ----------------------------- 51.9/204.1 MB 3.4 MB/s eta 0:00:46\n",
      "   ---------- ----------------------------- 52.7/204.1 MB 3.3 MB/s eta 0:00:46\n",
      "   ---------- ----------------------------- 53.5/204.1 MB 3.4 MB/s eta 0:00:45\n",
      "   ---------- ----------------------------- 54.0/204.1 MB 3.4 MB/s eta 0:00:45\n",
      "   ---------- ----------------------------- 54.5/204.1 MB 3.4 MB/s eta 0:00:45\n",
      "   ---------- ----------------------------- 55.3/204.1 MB 3.3 MB/s eta 0:00:45\n",
      "   ---------- ----------------------------- 56.1/204.1 MB 3.3 MB/s eta 0:00:45\n",
      "   ----------- ---------------------------- 56.9/204.1 MB 3.3 MB/s eta 0:00:44\n",
      "   ----------- ---------------------------- 57.7/204.1 MB 3.4 MB/s eta 0:00:44\n",
      "   ----------- ---------------------------- 58.2/204.1 MB 3.4 MB/s eta 0:00:44\n",
      "   ----------- ---------------------------- 59.2/204.1 MB 3.4 MB/s eta 0:00:44\n",
      "   ----------- ---------------------------- 60.0/204.1 MB 3.4 MB/s eta 0:00:43\n",
      "   ----------- ---------------------------- 60.8/204.1 MB 3.4 MB/s eta 0:00:43\n",
      "   ------------ --------------------------- 61.6/204.1 MB 3.4 MB/s eta 0:00:43\n",
      "   ------------ --------------------------- 62.4/204.1 MB 3.4 MB/s eta 0:00:42\n",
      "   ------------ --------------------------- 63.2/204.1 MB 3.4 MB/s eta 0:00:42\n",
      "   ------------ --------------------------- 63.7/204.1 MB 3.4 MB/s eta 0:00:42\n",
      "   ------------ --------------------------- 64.7/204.1 MB 3.4 MB/s eta 0:00:42\n",
      "   ------------ --------------------------- 65.5/204.1 MB 3.4 MB/s eta 0:00:41\n",
      "   ------------ --------------------------- 66.1/204.1 MB 3.4 MB/s eta 0:00:41\n",
      "   ------------- -------------------------- 66.8/204.1 MB 3.4 MB/s eta 0:00:41\n",
      "   ------------- -------------------------- 67.9/204.1 MB 3.4 MB/s eta 0:00:40\n",
      "   ------------- -------------------------- 68.4/204.1 MB 3.4 MB/s eta 0:00:40\n",
      "   ------------- -------------------------- 69.5/204.1 MB 3.4 MB/s eta 0:00:40\n",
      "   ------------- -------------------------- 70.0/204.1 MB 3.4 MB/s eta 0:00:40\n",
      "   ------------- -------------------------- 70.8/204.1 MB 3.4 MB/s eta 0:00:40\n",
      "   -------------- ------------------------- 71.6/204.1 MB 3.4 MB/s eta 0:00:39\n",
      "   -------------- ------------------------- 72.1/204.1 MB 3.4 MB/s eta 0:00:39\n",
      "   -------------- ------------------------- 72.6/204.1 MB 3.4 MB/s eta 0:00:39\n",
      "   -------------- ------------------------- 73.7/204.1 MB 3.4 MB/s eta 0:00:39\n",
      "   -------------- ------------------------- 74.4/204.1 MB 3.4 MB/s eta 0:00:38\n",
      "   -------------- ------------------------- 75.2/204.1 MB 3.4 MB/s eta 0:00:38\n",
      "   -------------- ------------------------- 75.8/204.1 MB 3.4 MB/s eta 0:00:38\n",
      "   --------------- ------------------------ 76.5/204.1 MB 3.4 MB/s eta 0:00:38\n",
      "   --------------- ------------------------ 76.8/204.1 MB 3.4 MB/s eta 0:00:38\n",
      "   --------------- ------------------------ 77.9/204.1 MB 3.4 MB/s eta 0:00:37\n",
      "   --------------- ------------------------ 78.6/204.1 MB 3.4 MB/s eta 0:00:37\n",
      "   --------------- ------------------------ 79.2/204.1 MB 3.4 MB/s eta 0:00:37\n",
      "   --------------- ------------------------ 80.2/204.1 MB 3.4 MB/s eta 0:00:37\n",
      "   --------------- ------------------------ 80.7/204.1 MB 3.4 MB/s eta 0:00:37\n",
      "   --------------- ------------------------ 81.5/204.1 MB 3.4 MB/s eta 0:00:36\n",
      "   ---------------- ----------------------- 82.3/204.1 MB 3.4 MB/s eta 0:00:36\n",
      "   ---------------- ----------------------- 83.1/204.1 MB 3.4 MB/s eta 0:00:36\n",
      "   ---------------- ----------------------- 83.9/204.1 MB 3.4 MB/s eta 0:00:36\n",
      "   ---------------- ----------------------- 84.9/204.1 MB 3.4 MB/s eta 0:00:35\n",
      "   ---------------- ----------------------- 85.7/204.1 MB 3.4 MB/s eta 0:00:35\n",
      "   ---------------- ----------------------- 86.2/204.1 MB 3.4 MB/s eta 0:00:35\n",
      "   ----------------- ---------------------- 87.0/204.1 MB 3.4 MB/s eta 0:00:35\n",
      "   ----------------- ---------------------- 87.8/204.1 MB 3.4 MB/s eta 0:00:34\n",
      "   ----------------- ---------------------- 88.6/204.1 MB 3.4 MB/s eta 0:00:34\n",
      "   ----------------- ---------------------- 89.4/204.1 MB 3.4 MB/s eta 0:00:34\n",
      "   ----------------- ---------------------- 90.4/204.1 MB 3.5 MB/s eta 0:00:33\n",
      "   ----------------- ---------------------- 91.2/204.1 MB 3.5 MB/s eta 0:00:33\n",
      "   ------------------ --------------------- 92.3/204.1 MB 3.5 MB/s eta 0:00:33\n",
      "   ------------------ --------------------- 93.1/204.1 MB 3.5 MB/s eta 0:00:32\n",
      "   ------------------ --------------------- 93.6/204.1 MB 3.5 MB/s eta 0:00:32\n",
      "   ------------------ --------------------- 94.4/204.1 MB 3.5 MB/s eta 0:00:32\n",
      "   ------------------ --------------------- 95.4/204.1 MB 3.5 MB/s eta 0:00:32\n",
      "   ------------------ --------------------- 96.2/204.1 MB 3.5 MB/s eta 0:00:32\n",
      "   ------------------- -------------------- 97.0/204.1 MB 3.5 MB/s eta 0:00:31\n",
      "   ------------------- -------------------- 97.8/204.1 MB 3.5 MB/s eta 0:00:31\n",
      "   ------------------- -------------------- 98.6/204.1 MB 3.5 MB/s eta 0:00:31\n",
      "   ------------------- -------------------- 99.4/204.1 MB 3.5 MB/s eta 0:00:31\n",
      "   ------------------- -------------------- 100.4/204.1 MB 3.5 MB/s eta 0:00:30\n",
      "   ------------------- -------------------- 100.9/204.1 MB 3.5 MB/s eta 0:00:30\n",
      "   ------------------- -------------------- 102.0/204.1 MB 3.5 MB/s eta 0:00:30\n",
      "   -------------------- ------------------- 102.8/204.1 MB 3.5 MB/s eta 0:00:29\n",
      "   -------------------- ------------------- 103.3/204.1 MB 3.5 MB/s eta 0:00:29\n",
      "   -------------------- ------------------- 104.1/204.1 MB 3.5 MB/s eta 0:00:29\n",
      "   -------------------- ------------------- 104.9/204.1 MB 3.5 MB/s eta 0:00:29\n",
      "   -------------------- ------------------- 104.9/204.1 MB 3.5 MB/s eta 0:00:29\n",
      "   -------------------- ------------------- 106.2/204.1 MB 3.5 MB/s eta 0:00:28\n",
      "   -------------------- ------------------- 107.0/204.1 MB 3.5 MB/s eta 0:00:28\n",
      "   --------------------- ------------------ 107.7/204.1 MB 3.5 MB/s eta 0:00:28\n",
      "   --------------------- ------------------ 108.5/204.1 MB 3.6 MB/s eta 0:00:27\n",
      "   --------------------- ------------------ 109.1/204.1 MB 3.6 MB/s eta 0:00:27\n",
      "   --------------------- ------------------ 109.8/204.1 MB 3.6 MB/s eta 0:00:27\n",
      "   --------------------- ------------------ 110.6/204.1 MB 3.6 MB/s eta 0:00:27\n",
      "   --------------------- ------------------ 111.1/204.1 MB 3.6 MB/s eta 0:00:26\n",
      "   --------------------- ------------------ 112.2/204.1 MB 3.6 MB/s eta 0:00:26\n",
      "   ---------------------- ----------------- 113.0/204.1 MB 3.6 MB/s eta 0:00:26\n",
      "   ---------------------- ----------------- 113.8/204.1 MB 3.6 MB/s eta 0:00:25\n",
      "   ---------------------- ----------------- 114.6/204.1 MB 3.6 MB/s eta 0:00:25\n",
      "   ---------------------- ----------------- 115.3/204.1 MB 3.6 MB/s eta 0:00:25\n",
      "   ---------------------- ----------------- 116.1/204.1 MB 3.6 MB/s eta 0:00:25\n",
      "   ---------------------- ----------------- 116.9/204.1 MB 3.6 MB/s eta 0:00:25\n",
      "   ----------------------- ---------------- 117.4/204.1 MB 3.6 MB/s eta 0:00:24\n",
      "   ----------------------- ---------------- 118.2/204.1 MB 3.6 MB/s eta 0:00:24\n",
      "   ----------------------- ---------------- 119.3/204.1 MB 3.6 MB/s eta 0:00:24\n",
      "   ----------------------- ---------------- 120.1/204.1 MB 3.6 MB/s eta 0:00:24\n",
      "   ----------------------- ---------------- 120.8/204.1 MB 3.6 MB/s eta 0:00:23\n",
      "   ----------------------- ---------------- 121.6/204.1 MB 3.6 MB/s eta 0:00:23\n",
      "   ----------------------- ---------------- 122.4/204.1 MB 3.6 MB/s eta 0:00:23\n",
      "   ------------------------ --------------- 122.9/204.1 MB 3.6 MB/s eta 0:00:23\n",
      "   ------------------------ --------------- 124.0/204.1 MB 3.6 MB/s eta 0:00:23\n",
      "   ------------------------ --------------- 124.5/204.1 MB 3.7 MB/s eta 0:00:22\n",
      "   ------------------------ --------------- 125.6/204.1 MB 3.7 MB/s eta 0:00:22\n",
      "   ------------------------ --------------- 126.6/204.1 MB 3.7 MB/s eta 0:00:22\n",
      "   ------------------------ --------------- 127.4/204.1 MB 3.7 MB/s eta 0:00:21\n",
      "   ------------------------- -------------- 127.9/204.1 MB 3.7 MB/s eta 0:00:21\n",
      "   ------------------------- -------------- 128.7/204.1 MB 3.7 MB/s eta 0:00:21\n",
      "   ------------------------- -------------- 129.5/204.1 MB 3.7 MB/s eta 0:00:21\n",
      "   ------------------------- -------------- 130.0/204.1 MB 3.6 MB/s eta 0:00:21\n",
      "   ------------------------- -------------- 130.8/204.1 MB 3.6 MB/s eta 0:00:21\n",
      "   ------------------------- -------------- 131.6/204.1 MB 3.6 MB/s eta 0:00:20\n",
      "   ------------------------- -------------- 132.4/204.1 MB 3.6 MB/s eta 0:00:20\n",
      "   -------------------------- ------------- 133.2/204.1 MB 3.6 MB/s eta 0:00:20\n",
      "   -------------------------- ------------- 133.2/204.1 MB 3.6 MB/s eta 0:00:20\n",
      "   -------------------------- ------------- 134.5/204.1 MB 3.6 MB/s eta 0:00:20\n",
      "   -------------------------- ------------- 135.5/204.1 MB 3.6 MB/s eta 0:00:19\n",
      "   -------------------------- ------------- 136.3/204.1 MB 3.6 MB/s eta 0:00:19\n",
      "   -------------------------- ------------- 137.1/204.1 MB 3.6 MB/s eta 0:00:19\n",
      "   -------------------------- ------------- 137.6/204.1 MB 3.6 MB/s eta 0:00:19\n",
      "   --------------------------- ------------ 138.7/204.1 MB 3.6 MB/s eta 0:00:18\n",
      "   --------------------------- ------------ 139.2/204.1 MB 3.6 MB/s eta 0:00:18\n",
      "   --------------------------- ------------ 140.0/204.1 MB 3.7 MB/s eta 0:00:18\n",
      "   --------------------------- ------------ 140.8/204.1 MB 3.6 MB/s eta 0:00:18\n",
      "   --------------------------- ------------ 141.6/204.1 MB 3.7 MB/s eta 0:00:18\n",
      "   --------------------------- ------------ 142.3/204.1 MB 3.7 MB/s eta 0:00:17\n",
      "   ---------------------------- ----------- 143.1/204.1 MB 3.7 MB/s eta 0:00:17\n",
      "   ---------------------------- ----------- 143.9/204.1 MB 3.7 MB/s eta 0:00:17\n",
      "   ---------------------------- ----------- 144.7/204.1 MB 3.7 MB/s eta 0:00:17\n",
      "   ---------------------------- ----------- 145.2/204.1 MB 3.7 MB/s eta 0:00:17\n",
      "   ---------------------------- ----------- 146.3/204.1 MB 3.7 MB/s eta 0:00:16\n",
      "   ---------------------------- ----------- 147.1/204.1 MB 3.7 MB/s eta 0:00:16\n",
      "   ----------------------------- ---------- 148.1/204.1 MB 3.7 MB/s eta 0:00:16\n",
      "   ----------------------------- ---------- 148.9/204.1 MB 3.7 MB/s eta 0:00:16\n",
      "   ----------------------------- ---------- 149.7/204.1 MB 3.7 MB/s eta 0:00:15\n",
      "   ----------------------------- ---------- 150.7/204.1 MB 3.7 MB/s eta 0:00:15\n",
      "   ----------------------------- ---------- 151.5/204.1 MB 3.7 MB/s eta 0:00:15\n",
      "   ----------------------------- ---------- 152.0/204.1 MB 3.7 MB/s eta 0:00:15\n",
      "   ------------------------------ --------- 153.1/204.1 MB 3.7 MB/s eta 0:00:14\n",
      "   ------------------------------ --------- 153.6/204.1 MB 3.7 MB/s eta 0:00:14\n",
      "   ------------------------------ --------- 154.4/204.1 MB 3.7 MB/s eta 0:00:14\n",
      "   ------------------------------ --------- 155.2/204.1 MB 3.7 MB/s eta 0:00:14\n",
      "   ------------------------------ --------- 156.0/204.1 MB 3.7 MB/s eta 0:00:14\n",
      "   ------------------------------ --------- 156.8/204.1 MB 3.7 MB/s eta 0:00:13\n",
      "   ------------------------------ --------- 157.5/204.1 MB 3.7 MB/s eta 0:00:13\n",
      "   ------------------------------- -------- 158.3/204.1 MB 3.7 MB/s eta 0:00:13\n",
      "   ------------------------------- -------- 158.9/204.1 MB 3.7 MB/s eta 0:00:13\n",
      "   ------------------------------- -------- 159.6/204.1 MB 3.7 MB/s eta 0:00:13\n",
      "   ------------------------------- -------- 160.7/204.1 MB 3.7 MB/s eta 0:00:12\n",
      "   ------------------------------- -------- 161.2/204.1 MB 3.7 MB/s eta 0:00:12\n",
      "   ------------------------------- -------- 161.7/204.1 MB 3.7 MB/s eta 0:00:12\n",
      "   ------------------------------- -------- 162.5/204.1 MB 3.7 MB/s eta 0:00:12\n",
      "   -------------------------------- ------- 163.3/204.1 MB 3.7 MB/s eta 0:00:12\n",
      "   -------------------------------- ------- 164.4/204.1 MB 3.7 MB/s eta 0:00:11\n",
      "   -------------------------------- ------- 165.2/204.1 MB 3.7 MB/s eta 0:00:11\n",
      "   -------------------------------- ------- 165.9/204.1 MB 3.7 MB/s eta 0:00:11\n",
      "   -------------------------------- ------- 166.7/204.1 MB 3.7 MB/s eta 0:00:11\n",
      "   -------------------------------- ------- 167.5/204.1 MB 3.7 MB/s eta 0:00:10\n",
      "   -------------------------------- ------- 168.3/204.1 MB 3.7 MB/s eta 0:00:10\n",
      "   --------------------------------- ------ 169.1/204.1 MB 3.7 MB/s eta 0:00:10\n",
      "   --------------------------------- ------ 169.9/204.1 MB 3.7 MB/s eta 0:00:10\n",
      "   --------------------------------- ------ 170.7/204.1 MB 3.7 MB/s eta 0:00:10\n",
      "   --------------------------------- ------ 171.4/204.1 MB 3.7 MB/s eta 0:00:09\n",
      "   --------------------------------- ------ 172.2/204.1 MB 3.7 MB/s eta 0:00:09\n",
      "   --------------------------------- ------ 173.0/204.1 MB 3.7 MB/s eta 0:00:09\n",
      "   ---------------------------------- ----- 173.8/204.1 MB 3.7 MB/s eta 0:00:09\n",
      "   ---------------------------------- ----- 174.6/204.1 MB 3.7 MB/s eta 0:00:08\n",
      "   ---------------------------------- ----- 175.6/204.1 MB 3.7 MB/s eta 0:00:08\n",
      "   ---------------------------------- ----- 176.2/204.1 MB 3.7 MB/s eta 0:00:08\n",
      "   ---------------------------------- ----- 176.7/204.1 MB 3.7 MB/s eta 0:00:08\n",
      "   ---------------------------------- ----- 177.7/204.1 MB 3.7 MB/s eta 0:00:08\n",
      "   ---------------------------------- ----- 178.5/204.1 MB 3.7 MB/s eta 0:00:07\n",
      "   ----------------------------------- ---- 179.3/204.1 MB 3.7 MB/s eta 0:00:07\n",
      "   ----------------------------------- ---- 180.1/204.1 MB 3.7 MB/s eta 0:00:07\n",
      "   ----------------------------------- ---- 180.9/204.1 MB 3.7 MB/s eta 0:00:07\n",
      "   ----------------------------------- ---- 181.7/204.1 MB 3.7 MB/s eta 0:00:07\n",
      "   ----------------------------------- ---- 182.5/204.1 MB 3.7 MB/s eta 0:00:06\n",
      "   ----------------------------------- ---- 183.0/204.1 MB 3.7 MB/s eta 0:00:06\n",
      "   ------------------------------------ --- 183.8/204.1 MB 3.7 MB/s eta 0:00:06\n",
      "   ------------------------------------ --- 184.5/204.1 MB 3.7 MB/s eta 0:00:06\n",
      "   ------------------------------------ --- 185.3/204.1 MB 3.7 MB/s eta 0:00:06\n",
      "   ------------------------------------ --- 186.1/204.1 MB 3.7 MB/s eta 0:00:05\n",
      "   ------------------------------------ --- 186.9/204.1 MB 3.7 MB/s eta 0:00:05\n",
      "   ------------------------------------ --- 187.7/204.1 MB 3.7 MB/s eta 0:00:05\n",
      "   ------------------------------------ --- 188.7/204.1 MB 3.8 MB/s eta 0:00:05\n",
      "   ------------------------------------- -- 189.3/204.1 MB 3.7 MB/s eta 0:00:04\n",
      "   ------------------------------------- -- 190.1/204.1 MB 3.7 MB/s eta 0:00:04\n",
      "   ------------------------------------- -- 190.6/204.1 MB 3.7 MB/s eta 0:00:04\n",
      "   ------------------------------------- -- 191.6/204.1 MB 3.7 MB/s eta 0:00:04\n",
      "   ------------------------------------- -- 192.4/204.1 MB 3.7 MB/s eta 0:00:04\n",
      "   ------------------------------------- -- 193.2/204.1 MB 3.7 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 194.0/204.1 MB 3.7 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 194.8/204.1 MB 3.7 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 195.6/204.1 MB 3.7 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 196.3/204.1 MB 3.7 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 196.9/204.1 MB 3.7 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 197.7/204.1 MB 3.7 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 198.7/204.1 MB 3.7 MB/s eta 0:00:02\n",
      "   ---------------------------------------  199.5/204.1 MB 3.7 MB/s eta 0:00:02\n",
      "   ---------------------------------------  200.5/204.1 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  201.6/204.1 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  202.4/204.1 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  203.2/204.1 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  203.9/204.1 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  203.9/204.1 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  203.9/204.1 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 204.1/204.1 MB 3.7 MB/s eta 0:00:00\n",
      "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.5/6.2 MB 3.4 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 1.3/6.2 MB 3.7 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 2.1/6.2 MB 3.7 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 2.6/6.2 MB 3.4 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 3.4/6.2 MB 3.5 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 4.2/6.2 MB 3.4 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 4.7/6.2 MB 3.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 5.5/6.2 MB 3.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.0/6.2 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.2/6.2 MB 3.3 MB/s eta 0:00:00\n",
      "Downloading transformers-4.48.1-py3-none-any.whl (9.7 MB)\n",
      "   ---------------------------------------- 0.0/9.7 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.5/9.7 MB 2.4 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 1.0/9.7 MB 3.0 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 1.8/9.7 MB 3.0 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 2.9/9.7 MB 3.6 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 3.7/9.7 MB 3.6 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 4.5/9.7 MB 3.7 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 5.5/9.7 MB 3.9 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 6.0/9.7 MB 3.7 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 6.6/9.7 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 7.6/9.7 MB 3.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 8.1/9.7 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 8.9/9.7 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.7/9.7 MB 3.7 MB/s eta 0:00:00\n",
      "Downloading sentence_transformers-3.4.1-py3-none-any.whl (275 kB)\n",
      "Downloading huggingface_hub-0.28.0-py3-none-any.whl (464 kB)\n",
      "Downloading safetensors-0.5.2-cp38-abi3-win_amd64.whl (303 kB)\n",
      "Downloading tokenizers-0.21.0-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ----------------- ---------------------- 1.0/2.4 MB 5.0 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.8/2.4 MB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 3.8 MB/s eta 0:00:00\n",
      "Installing collected packages: sympy, safetensors, torch, huggingface-hub, tokenizers, transformers, sentence-transformers\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.2\n",
      "    Uninstalling sympy-1.13.2:\n",
      "      Successfully uninstalled sympy-1.13.2\n",
      "Successfully installed huggingface-hub-0.28.0 safetensors-0.5.2 sentence-transformers-3.4.1 sympy-1.13.1 tokenizers-0.21.0 torch-2.6.0 transformers-4.48.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script isympy.exe is installed in 'c:\\Users\\deeks\\anaconda3\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts torchfrtrace.exe and torchrun.exe are installed in 'c:\\Users\\deeks\\anaconda3\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script huggingface-cli.exe is installed in 'c:\\Users\\deeks\\anaconda3\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script transformers-cli.exe is installed in 'c:\\Users\\deeks\\anaconda3\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "pip install torch transformers sentence-transformers langchain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a71d5f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\deeks\\anaconda3\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from torch) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e38e973a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "06833765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\deeks\\anaconda3\\lib\\site-packages (0.0.284)\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.3.17-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.3.16-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from langchain) (2.0.34)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from langchain) (3.10.5)\n",
      "Collecting langchain-core<0.4.0,>=0.3.33 (from langchain)\n",
      "  Downloading langchain_core-0.3.33-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting langchain-text-splitters<0.4.0,>=0.3.3 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.3.5-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting langsmith<0.4,>=0.1.17 (from langchain)\n",
      "  Downloading langsmith-0.3.3-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy<3,>=1.26.2 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from langchain) (2.8.2)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from langchain-community) (0.5.14)\n",
      "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain-community)\n",
      "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.11.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.0)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.33->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.33->langchain) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.33->langchain) (4.11.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.27.0)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Downloading orjson-3.10.15-cp312-cp312-win_amd64.whl.metadata (42 kB)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.20.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2024.12.14)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.33->langchain) (2.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Downloading langchain-0.3.17-py3-none-any.whl (1.0 MB)\n",
      "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 0.3/1.0 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 0.3/1.0 MB ? eta -:--:--\n",
      "   -------------------- ------------------- 0.5/1.0 MB 672.2 kB/s eta 0:00:01\n",
      "   ------------------------------- -------- 0.8/1.0 MB 799.2 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.0/1.0 MB 825.5 kB/s eta 0:00:00\n",
      "Downloading langchain_community-0.3.16-py3-none-any.whl (2.5 MB)\n",
      "   ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.5 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 0.5/2.5 MB 799.2 kB/s eta 0:00:03\n",
      "   -------- ------------------------------- 0.5/2.5 MB 799.2 kB/s eta 0:00:03\n",
      "   ------------ --------------------------- 0.8/2.5 MB 729.2 kB/s eta 0:00:03\n",
      "   ------------ --------------------------- 0.8/2.5 MB 729.2 kB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 1.0/2.5 MB 825.2 kB/s eta 0:00:02\n",
      "   -------------------- ------------------- 1.3/2.5 MB 838.9 kB/s eta 0:00:02\n",
      "   -------------------- ------------------- 1.3/2.5 MB 838.9 kB/s eta 0:00:02\n",
      "   -------------------- ------------------- 1.3/2.5 MB 838.9 kB/s eta 0:00:02\n",
      "   ------------------------- -------------- 1.6/2.5 MB 660.6 kB/s eta 0:00:02\n",
      "   ------------------------- -------------- 1.6/2.5 MB 660.6 kB/s eta 0:00:02\n",
      "   ------------------------- -------------- 1.6/2.5 MB 660.6 kB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 1.8/2.5 MB 653.7 kB/s eta 0:00:02\n",
      "   --------------------------------- ------ 2.1/2.5 MB 703.3 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 2.1/2.5 MB 703.3 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.4/2.5 MB 681.3 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.4/2.5 MB 681.3 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.4/2.5 MB 681.3 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.5/2.5 MB 592.8 kB/s eta 0:00:00\n",
      "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading langchain_core-0.3.33-py3-none-any.whl (412 kB)\n",
      "Downloading langchain_text_splitters-0.3.5-py3-none-any.whl (31 kB)\n",
      "Downloading langsmith-0.3.3-py3-none-any.whl (333 kB)\n",
      "Downloading orjson-3.10.15-cp312-cp312-win_amd64.whl (133 kB)\n",
      "Installing collected packages: orjson, httpx-sse, langsmith, langchain-core, langchain-text-splitters, langchain, langchain-community\n",
      "  Attempting uninstall: langsmith\n",
      "    Found existing installation: langsmith 0.0.92\n",
      "    Uninstalling langsmith-0.0.92:\n",
      "      Successfully uninstalled langsmith-0.0.92\n",
      "  Attempting uninstall: langchain\n",
      "    Found existing installation: langchain 0.0.284\n",
      "    Uninstalling langchain-0.0.284:\n",
      "      Successfully uninstalled langchain-0.0.284\n",
      "Successfully installed httpx-sse-0.4.0 langchain-0.3.17 langchain-community-0.3.16 langchain-core-0.3.33 langchain-text-splitters-0.3.5 langsmith-0.3.3 orjson-3.10.15\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script langchain-server.exe is installed in 'c:\\Users\\deeks\\anaconda3\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade langchain langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e68108c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet  langchain-google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b86a8f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = 'getpass.getpass(\"AIzaSyDz2Dt2LonIglhqMihyIJiAFeIgT82p9XA\")'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "74856bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.05168594419956207, -0.030764883384108543, -0.03062233328819275, -0.02802734263241291, 0.01813093200325966]\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/embedding-001\",  # Corrected model name\n",
    "    google_api_key=\"AIzaSyDz2Dt2LonIglhqMihyIJiAFeIgT82p9XA\"  # Ensure correct key format\n",
    ")\n",
    "\n",
    "vector = embeddings.embed_query(\"hello, world!\")\n",
    "print(vector[:5])  # Print first 5 values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d802a630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 768)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors = embeddings.embed_documents(\n",
    "    [\n",
    "        \"Today is Monday\",\n",
    "        \"Today is Tuesday\",\n",
    "        \"Today is April Fools day\",\n",
    "    ]\n",
    ")\n",
    "len(vectors), len(vectors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9a2fa142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on GoogleGenerativeAIEmbeddings in module langchain_google_genai.embeddings object:\n",
      "\n",
      "class GoogleGenerativeAIEmbeddings(pydantic.main.BaseModel, langchain_core.embeddings.embeddings.Embeddings)\n",
      " |  GoogleGenerativeAIEmbeddings(*, client: Any = None, model: str, task_type: Optional[str] = None, google_api_key: Optional[pydantic.types.SecretStr] = None, credentials: Any = None, client_options: Optional[Dict] = None, transport: Optional[str] = None, request_options: Optional[Dict] = None) -> None\n",
      " |\n",
      " |  `Google Generative AI Embeddings`.\n",
      " |\n",
      " |  To use, you must have either:\n",
      " |\n",
      " |      1. The ``GOOGLE_API_KEY`` environment variable set with your API key, or\n",
      " |      2. Pass your API key using the google_api_key kwarg\n",
      " |      to the GoogleGenerativeAIEmbeddings constructor.\n",
      " |\n",
      " |  Example:\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
      " |\n",
      " |          embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
      " |          embeddings.embed_query(\"What's our Q1 revenue?\")\n",
      " |\n",
      " |  Method resolution order:\n",
      " |      GoogleGenerativeAIEmbeddings\n",
      " |      pydantic.main.BaseModel\n",
      " |      langchain_core.embeddings.embeddings.Embeddings\n",
      " |      abc.ABC\n",
      " |      builtins.object\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  embed_documents(self, texts: List[str], *, batch_size: int = 100, task_type: Optional[str] = None, titles: Optional[List[str]] = None, output_dimensionality: Optional[int] = None) -> List[List[float]]\n",
      " |      Embed a list of strings. Google Generative AI currently\n",
      " |      sets a max batch size of 100 strings.\n",
      " |\n",
      " |      Args:\n",
      " |          texts: List[str] The list of strings to embed.\n",
      " |          batch_size: [int] The batch size of embeddings to send to the model\n",
      " |          task_type: task_type (https://ai.google.dev/api/rest/v1/TaskType)\n",
      " |          titles: An optional list of titles for texts provided.\n",
      " |          Only applicable when TaskType is RETRIEVAL_DOCUMENT.\n",
      " |          output_dimensionality: Optional reduced dimension for the output embedding.\n",
      " |          https://ai.google.dev/api/rest/v1/models/batchEmbedContents#EmbedContentRequest\n",
      " |      Returns:\n",
      " |          List of embeddings, one for each text.\n",
      " |\n",
      " |  embed_query(self, text: str, task_type: Optional[str] = None, title: Optional[str] = None, output_dimensionality: Optional[int] = None) -> List[float]\n",
      " |      Embed a text.\n",
      " |\n",
      " |      Args:\n",
      " |          text: The text to embed.\n",
      " |          task_type: task_type (https://ai.google.dev/api/rest/v1/TaskType)\n",
      " |          title: An optional title for the text.\n",
      " |          Only applicable when TaskType is RETRIEVAL_DOCUMENT.\n",
      " |          output_dimensionality: Optional reduced dimension for the output embedding.\n",
      " |          https://ai.google.dev/api/rest/v1/models/batchEmbedContents#EmbedContentRequest\n",
      " |\n",
      " |      Returns:\n",
      " |          Embedding for the text.\n",
      " |\n",
      " |  validate_environment(self) -> Self\n",
      " |      Validates params and passes them to google-generativeai package.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |\n",
      " |  __abstractmethods__ = frozenset()\n",
      " |\n",
      " |  __annotations__ = {'client': typing.Any, 'client_options': typing.Opti...\n",
      " |\n",
      " |  __class_vars__ = set()\n",
      " |\n",
      " |  __private_attributes__ = {}\n",
      " |\n",
      " |  __pydantic_complete__ = True\n",
      " |\n",
      " |  __pydantic_core_schema__ = {'function': {'function': <function GoogleG...\n",
      " |\n",
      " |  __pydantic_custom_init__ = False\n",
      " |\n",
      " |  __pydantic_decorators__ = DecoratorInfos(validators={}, field_validato...\n",
      " |\n",
      " |  __pydantic_generic_metadata__ = {'args': (), 'origin': None, 'paramete...\n",
      " |\n",
      " |  __pydantic_parent_namespace__ = {'Any': <pydantic._internal._model_con...\n",
      " |\n",
      " |  __pydantic_post_init__ = None\n",
      " |\n",
      " |  __pydantic_serializer__ = SchemaSerializer(serializer=Model(\n",
      " |      Model...\n",
      " |\n",
      " |  __pydantic_validator__ = SchemaValidator(title=\"GoogleGenerativeAIEmbe...\n",
      " |\n",
      " |  __signature__ = <Signature (*, client: Any = None, model: str, t... re...\n",
      " |\n",
      " |  model_computed_fields = {}\n",
      " |\n",
      " |  model_config = {}\n",
      " |\n",
      " |  model_fields = {'client': FieldInfo(annotation=Any, required=False, de...\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pydantic.main.BaseModel:\n",
      " |\n",
      " |  __copy__(self) -> 'Self'\n",
      " |      Returns a shallow copy of the model.\n",
      " |\n",
      " |  __deepcopy__(self, memo: 'dict[int, Any] | None' = None) -> 'Self'\n",
      " |      Returns a deep copy of the model.\n",
      " |\n",
      " |  __delattr__(self, item: 'str') -> 'Any'\n",
      " |      Implement delattr(self, name).\n",
      " |\n",
      " |  __eq__(self, other: 'Any') -> 'bool'\n",
      " |      Return self==value.\n",
      " |\n",
      " |  __getattr__(self, item: 'str') -> 'Any'\n",
      " |\n",
      " |  __getstate__(self) -> 'dict[Any, Any]'\n",
      " |      Helper for pickle.\n",
      " |\n",
      " |  __init__(self, /, **data: 'Any') -> 'None'\n",
      " |      Create a new model by parsing and validating input data from keyword arguments.\n",
      " |\n",
      " |      Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n",
      " |      validated to form a valid model.\n",
      " |\n",
      " |      `self` is explicitly positional-only to allow `self` as a field name.\n",
      " |\n",
      " |  __iter__(self) -> 'TupleGenerator'\n",
      " |      So `dict(model)` works.\n",
      " |\n",
      " |  __pretty__(self, fmt: 'typing.Callable[[Any], Any]', **kwargs: 'Any') -> 'typing.Generator[Any, None, None]' from pydantic._internal._repr.Representation\n",
      " |      Used by devtools (https://python-devtools.helpmanual.io/) to pretty print objects.\n",
      " |\n",
      " |  __repr__(self) -> 'str'\n",
      " |      Return repr(self).\n",
      " |\n",
      " |  __repr_args__(self) -> '_repr.ReprArgs'\n",
      " |\n",
      " |  __repr_name__(self) -> 'str' from pydantic._internal._repr.Representation\n",
      " |      Name of the instance's class, used in __repr__.\n",
      " |\n",
      " |  __repr_str__(self, join_str: 'str') -> 'str' from pydantic._internal._repr.Representation\n",
      " |\n",
      " |  __rich_repr__(self) -> 'RichReprResult' from pydantic._internal._repr.Representation\n",
      " |      Used by Rich (https://rich.readthedocs.io/en/stable/pretty.html) to pretty print objects.\n",
      " |\n",
      " |  __setattr__(self, name: 'str', value: 'Any') -> 'None'\n",
      " |      Implement setattr(self, name, value).\n",
      " |\n",
      " |  __setstate__(self, state: 'dict[Any, Any]') -> 'None'\n",
      " |\n",
      " |  __str__(self) -> 'str'\n",
      " |      Return str(self).\n",
      " |\n",
      " |  copy(self, *, include: 'AbstractSetIntStr | MappingIntStrAny | None' = None, exclude: 'AbstractSetIntStr | MappingIntStrAny | None' = None, update: 'Dict[str, Any] | None' = None, deep: 'bool' = False) -> 'Self'\n",
      " |      Returns a copy of the model.\n",
      " |\n",
      " |      !!! warning \"Deprecated\"\n",
      " |          This method is now deprecated; use `model_copy` instead.\n",
      " |\n",
      " |      If you need `include` or `exclude`, use:\n",
      " |\n",
      " |      ```py\n",
      " |      data = self.model_dump(include=include, exclude=exclude, round_trip=True)\n",
      " |      data = {**data, **(update or {})}\n",
      " |      copied = self.model_validate(data)\n",
      " |      ```\n",
      " |\n",
      " |      Args:\n",
      " |          include: Optional set or mapping specifying which fields to include in the copied model.\n",
      " |          exclude: Optional set or mapping specifying which fields to exclude in the copied model.\n",
      " |          update: Optional dictionary of field-value pairs to override field values in the copied model.\n",
      " |          deep: If True, the values of fields that are Pydantic models will be deep-copied.\n",
      " |\n",
      " |      Returns:\n",
      " |          A copy of the model with included, excluded and updated fields as specified.\n",
      " |\n",
      " |  dict(self, *, include: 'IncEx' = None, exclude: 'IncEx' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False) -> 'Dict[str, Any]'\n",
      " |\n",
      " |  json(self, *, include: 'IncEx' = None, exclude: 'IncEx' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, encoder: 'Callable[[Any], Any] | None' = PydanticUndefined, models_as_dict: 'bool' = PydanticUndefined, **dumps_kwargs: 'Any') -> 'str'\n",
      " |\n",
      " |  model_copy(self, *, update: 'dict[str, Any] | None' = None, deep: 'bool' = False) -> 'Self'\n",
      " |      Usage docs: https://docs.pydantic.dev/2.8/concepts/serialization/#model_copy\n",
      " |\n",
      " |      Returns a copy of the model.\n",
      " |\n",
      " |      Args:\n",
      " |          update: Values to change/add in the new model. Note: the data is not validated\n",
      " |              before creating the new model. You should trust this data.\n",
      " |          deep: Set to `True` to make a deep copy of the model.\n",
      " |\n",
      " |      Returns:\n",
      " |          New model instance.\n",
      " |\n",
      " |  model_dump(self, *, mode: \"Literal['json', 'python'] | str\" = 'python', include: 'IncEx' = None, exclude: 'IncEx' = None, context: 'Any | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, round_trip: 'bool' = False, warnings: \"bool | Literal['none', 'warn', 'error']\" = True, serialize_as_any: 'bool' = False) -> 'dict[str, Any]'\n",
      " |      Usage docs: https://docs.pydantic.dev/2.8/concepts/serialization/#modelmodel_dump\n",
      " |\n",
      " |      Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.\n",
      " |\n",
      " |      Args:\n",
      " |          mode: The mode in which `to_python` should run.\n",
      " |              If mode is 'json', the output will only contain JSON serializable types.\n",
      " |              If mode is 'python', the output may contain non-JSON-serializable Python objects.\n",
      " |          include: A set of fields to include in the output.\n",
      " |          exclude: A set of fields to exclude from the output.\n",
      " |          context: Additional context to pass to the serializer.\n",
      " |          by_alias: Whether to use the field's alias in the dictionary key if defined.\n",
      " |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n",
      " |          exclude_defaults: Whether to exclude fields that are set to their default value.\n",
      " |          exclude_none: Whether to exclude fields that have a value of `None`.\n",
      " |          round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n",
      " |          warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n",
      " |              \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n",
      " |          serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n",
      " |\n",
      " |      Returns:\n",
      " |          A dictionary representation of the model.\n",
      " |\n",
      " |  model_dump_json(self, *, indent: 'int | None' = None, include: 'IncEx' = None, exclude: 'IncEx' = None, context: 'Any | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, round_trip: 'bool' = False, warnings: \"bool | Literal['none', 'warn', 'error']\" = True, serialize_as_any: 'bool' = False) -> 'str'\n",
      " |      Usage docs: https://docs.pydantic.dev/2.8/concepts/serialization/#modelmodel_dump_json\n",
      " |\n",
      " |      Generates a JSON representation of the model using Pydantic's `to_json` method.\n",
      " |\n",
      " |      Args:\n",
      " |          indent: Indentation to use in the JSON output. If None is passed, the output will be compact.\n",
      " |          include: Field(s) to include in the JSON output.\n",
      " |          exclude: Field(s) to exclude from the JSON output.\n",
      " |          context: Additional context to pass to the serializer.\n",
      " |          by_alias: Whether to serialize using field aliases.\n",
      " |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n",
      " |          exclude_defaults: Whether to exclude fields that are set to their default value.\n",
      " |          exclude_none: Whether to exclude fields that have a value of `None`.\n",
      " |          round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n",
      " |          warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n",
      " |              \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n",
      " |          serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n",
      " |\n",
      " |      Returns:\n",
      " |          A JSON string representation of the model.\n",
      " |\n",
      " |  model_post_init(self, _BaseModel__context: 'Any') -> 'None'\n",
      " |      Override this method to perform additional initialization after `__init__` and `model_construct`.\n",
      " |      This is useful if you want to do some validation that requires the entire model to be initialized.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pydantic.main.BaseModel:\n",
      " |\n",
      " |  __class_getitem__(typevar_values: 'type[Any] | tuple[type[Any], ...]') -> 'type[BaseModel] | _forward_ref.PydanticRecursiveRef'\n",
      " |\n",
      " |  __get_pydantic_core_schema__(source: 'type[BaseModel]', handler: 'GetCoreSchemaHandler', /) -> 'CoreSchema'\n",
      " |      Hook into generating the model's CoreSchema.\n",
      " |\n",
      " |      Args:\n",
      " |          source: The class we are generating a schema for.\n",
      " |              This will generally be the same as the `cls` argument if this is a classmethod.\n",
      " |          handler: A callable that calls into Pydantic's internal CoreSchema generation logic.\n",
      " |\n",
      " |      Returns:\n",
      " |          A `pydantic-core` `CoreSchema`.\n",
      " |\n",
      " |  __get_pydantic_json_schema__(core_schema: 'CoreSchema', handler: 'GetJsonSchemaHandler', /) -> 'JsonSchemaValue'\n",
      " |      Hook into generating the model's JSON schema.\n",
      " |\n",
      " |      Args:\n",
      " |          core_schema: A `pydantic-core` CoreSchema.\n",
      " |              You can ignore this argument and call the handler with a new CoreSchema,\n",
      " |              wrap this CoreSchema (`{'type': 'nullable', 'schema': current_schema}`),\n",
      " |              or just call the handler with the original schema.\n",
      " |          handler: Call into Pydantic's internal JSON schema generation.\n",
      " |              This will raise a `pydantic.errors.PydanticInvalidForJsonSchema` if JSON schema\n",
      " |              generation fails.\n",
      " |              Since this gets called by `BaseModel.model_json_schema` you can override the\n",
      " |              `schema_generator` argument to that function to change JSON schema generation globally\n",
      " |              for a type.\n",
      " |\n",
      " |      Returns:\n",
      " |          A JSON schema, as a Python object.\n",
      " |\n",
      " |  __pydantic_init_subclass__(**kwargs: 'Any') -> 'None'\n",
      " |      This is intended to behave just like `__init_subclass__`, but is called by `ModelMetaclass`\n",
      " |      only after the class is actually fully initialized. In particular, attributes like `model_fields` will\n",
      " |      be present when this is called.\n",
      " |\n",
      " |      This is necessary because `__init_subclass__` will always be called by `type.__new__`,\n",
      " |      and it would require a prohibitively large refactor to the `ModelMetaclass` to ensure that\n",
      " |      `type.__new__` was called in such a manner that the class would already be sufficiently initialized.\n",
      " |\n",
      " |      This will receive the same `kwargs` that would be passed to the standard `__init_subclass__`, namely,\n",
      " |      any kwargs passed to the class definition that aren't used internally by pydantic.\n",
      " |\n",
      " |      Args:\n",
      " |          **kwargs: Any keyword arguments passed to the class definition that aren't used internally\n",
      " |              by pydantic.\n",
      " |\n",
      " |  construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -> 'Self'\n",
      " |\n",
      " |  from_orm(obj: 'Any') -> 'Self'\n",
      " |\n",
      " |  model_construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -> 'Self'\n",
      " |      Creates a new instance of the `Model` class with validated data.\n",
      " |\n",
      " |      Creates a new model setting `__dict__` and `__pydantic_fields_set__` from trusted or pre-validated data.\n",
      " |      Default values are respected, but no other validation is performed.\n",
      " |\n",
      " |      !!! note\n",
      " |          `model_construct()` generally respects the `model_config.extra` setting on the provided model.\n",
      " |          That is, if `model_config.extra == 'allow'`, then all extra passed values are added to the model instance's `__dict__`\n",
      " |          and `__pydantic_extra__` fields. If `model_config.extra == 'ignore'` (the default), then all extra passed values are ignored.\n",
      " |          Because no validation is performed with a call to `model_construct()`, having `model_config.extra == 'forbid'` does not result in\n",
      " |          an error if extra values are passed, but they will be ignored.\n",
      " |\n",
      " |      Args:\n",
      " |          _fields_set: The set of field names accepted for the Model instance.\n",
      " |          values: Trusted or pre-validated data dictionary.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new instance of the `Model` class with validated data.\n",
      " |\n",
      " |  model_json_schema(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}', schema_generator: 'type[GenerateJsonSchema]' = <class 'pydantic.json_schema.GenerateJsonSchema'>, mode: 'JsonSchemaMode' = 'validation') -> 'dict[str, Any]'\n",
      " |      Generates a JSON schema for a model class.\n",
      " |\n",
      " |      Args:\n",
      " |          by_alias: Whether to use attribute aliases or not.\n",
      " |          ref_template: The reference template.\n",
      " |          schema_generator: To override the logic used to generate the JSON schema, as a subclass of\n",
      " |              `GenerateJsonSchema` with your desired modifications\n",
      " |          mode: The mode in which to generate the schema.\n",
      " |\n",
      " |      Returns:\n",
      " |          The JSON schema for the given model class.\n",
      " |\n",
      " |  model_parametrized_name(params: 'tuple[type[Any], ...]') -> 'str'\n",
      " |      Compute the class name for parametrizations of generic classes.\n",
      " |\n",
      " |      This method can be overridden to achieve a custom naming scheme for generic BaseModels.\n",
      " |\n",
      " |      Args:\n",
      " |          params: Tuple of types of the class. Given a generic class\n",
      " |              `Model` with 2 type variables and a concrete model `Model[str, int]`,\n",
      " |              the value `(str, int)` would be passed to `params`.\n",
      " |\n",
      " |      Returns:\n",
      " |          String representing the new class where `params` are passed to `cls` as type variables.\n",
      " |\n",
      " |      Raises:\n",
      " |          TypeError: Raised when trying to generate concrete names for non-generic models.\n",
      " |\n",
      " |  model_rebuild(*, force: 'bool' = False, raise_errors: 'bool' = True, _parent_namespace_depth: 'int' = 2, _types_namespace: 'dict[str, Any] | None' = None) -> 'bool | None'\n",
      " |      Try to rebuild the pydantic-core schema for the model.\n",
      " |\n",
      " |      This may be necessary when one of the annotations is a ForwardRef which could not be resolved during\n",
      " |      the initial attempt to build the schema, and automatic rebuilding fails.\n",
      " |\n",
      " |      Args:\n",
      " |          force: Whether to force the rebuilding of the model schema, defaults to `False`.\n",
      " |          raise_errors: Whether to raise errors, defaults to `True`.\n",
      " |          _parent_namespace_depth: The depth level of the parent namespace, defaults to 2.\n",
      " |          _types_namespace: The types namespace, defaults to `None`.\n",
      " |\n",
      " |      Returns:\n",
      " |          Returns `None` if the schema is already \"complete\" and rebuilding was not required.\n",
      " |          If rebuilding _was_ required, returns `True` if rebuilding was successful, otherwise `False`.\n",
      " |\n",
      " |  model_validate(obj: 'Any', *, strict: 'bool | None' = None, from_attributes: 'bool | None' = None, context: 'Any | None' = None) -> 'Self'\n",
      " |      Validate a pydantic model instance.\n",
      " |\n",
      " |      Args:\n",
      " |          obj: The object to validate.\n",
      " |          strict: Whether to enforce types strictly.\n",
      " |          from_attributes: Whether to extract data from object attributes.\n",
      " |          context: Additional context to pass to the validator.\n",
      " |\n",
      " |      Raises:\n",
      " |          ValidationError: If the object could not be validated.\n",
      " |\n",
      " |      Returns:\n",
      " |          The validated model instance.\n",
      " |\n",
      " |  model_validate_json(json_data: 'str | bytes | bytearray', *, strict: 'bool | None' = None, context: 'Any | None' = None) -> 'Self'\n",
      " |      Usage docs: https://docs.pydantic.dev/2.8/concepts/json/#json-parsing\n",
      " |\n",
      " |      Validate the given JSON data against the Pydantic model.\n",
      " |\n",
      " |      Args:\n",
      " |          json_data: The JSON data to validate.\n",
      " |          strict: Whether to enforce types strictly.\n",
      " |          context: Extra variables to pass to the validator.\n",
      " |\n",
      " |      Returns:\n",
      " |          The validated Pydantic model.\n",
      " |\n",
      " |      Raises:\n",
      " |          ValueError: If `json_data` is not a JSON string.\n",
      " |\n",
      " |  model_validate_strings(obj: 'Any', *, strict: 'bool | None' = None, context: 'Any | None' = None) -> 'Self'\n",
      " |      Validate the given object with string data against the Pydantic model.\n",
      " |\n",
      " |      Args:\n",
      " |          obj: The object containing string data to validate.\n",
      " |          strict: Whether to enforce types strictly.\n",
      " |          context: Extra variables to pass to the validator.\n",
      " |\n",
      " |      Returns:\n",
      " |          The validated Pydantic model.\n",
      " |\n",
      " |  parse_file(path: 'str | Path', *, content_type: 'str | None' = None, encoding: 'str' = 'utf8', proto: 'DeprecatedParseProtocol | None' = None, allow_pickle: 'bool' = False) -> 'Self'\n",
      " |\n",
      " |  parse_obj(obj: 'Any') -> 'Self'\n",
      " |\n",
      " |  parse_raw(b: 'str | bytes', *, content_type: 'str | None' = None, encoding: 'str' = 'utf8', proto: 'DeprecatedParseProtocol | None' = None, allow_pickle: 'bool' = False) -> 'Self'\n",
      " |\n",
      " |  schema(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}') -> 'Dict[str, Any]'\n",
      " |\n",
      " |  schema_json(*, by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}', **dumps_kwargs: 'Any') -> 'str'\n",
      " |\n",
      " |  update_forward_refs(**localns: 'Any') -> 'None'\n",
      " |\n",
      " |  validate(value: 'Any') -> 'Self'\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from pydantic.main.BaseModel:\n",
      " |\n",
      " |  __fields_set__\n",
      " |\n",
      " |  model_extra\n",
      " |      Get extra fields set during validation.\n",
      " |\n",
      " |      Returns:\n",
      " |          A dictionary of extra fields, or `None` if `config.extra` is not set to `\"allow\"`.\n",
      " |\n",
      " |  model_fields_set\n",
      " |      Returns the set of fields that have been explicitly set on this model instance.\n",
      " |\n",
      " |      Returns:\n",
      " |          A set of strings representing the fields that have been set,\n",
      " |              i.e. that were not filled from defaults.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __pydantic_extra__\n",
      " |\n",
      " |  __pydantic_fields_set__\n",
      " |\n",
      " |  __pydantic_private__\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pydantic.main.BaseModel:\n",
      " |\n",
      " |  __hash__ = None\n",
      " |\n",
      " |  __pydantic_root_model__ = False\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain_core.embeddings.embeddings.Embeddings:\n",
      " |\n",
      " |  async aembed_documents(self, texts: list[str]) -> list[list[float]]\n",
      " |      Asynchronous Embed search docs.\n",
      " |\n",
      " |      Args:\n",
      " |          texts: List of text to embed.\n",
      " |\n",
      " |      Returns:\n",
      " |          List of embeddings.\n",
      " |\n",
      " |  async aembed_query(self, text: str) -> list[float]\n",
      " |      Asynchronous Embed query text.\n",
      " |\n",
      " |      Args:\n",
      " |          text: Text to embed.\n",
      " |\n",
      " |      Returns:\n",
      " |          Embedding.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4d0bfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=\"models/text-embedding-001\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "485e2a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=\"text-embedding-001\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fb8874b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\deeks\\anaconda3\\lib\\site-packages (0.3.17)\n",
      "Requirement already satisfied: langchain-google-genai in c:\\users\\deeks\\anaconda3\\lib\\site-packages (2.0.9)\n",
      "Requirement already satisfied: google-generativeai in c:\\users\\deeks\\anaconda3\\lib\\site-packages (0.8.4)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from langchain) (2.0.34)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from langchain) (3.10.5)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.33 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from langchain) (0.3.33)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from langchain) (0.3.5)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from langchain) (0.3.3)\n",
      "Requirement already satisfied: numpy<3,>=1.26.2 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from langchain) (2.8.2)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from langchain-google-genai) (1.2.0)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from google-generativeai) (0.6.15)\n",
      "Requirement already satisfied: google-api-core in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from google-generativeai) (2.24.1)\n",
      "Requirement already satisfied: google-api-python-client in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from google-generativeai) (2.160.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from google-generativeai) (2.38.0)\n",
      "Requirement already satisfied: protobuf in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from google-generativeai) (5.29.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from google-generativeai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from google-generativeai) (4.11.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.11.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from google-api-core->google-generativeai) (1.66.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.33->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.33->langchain) (24.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.27.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2024.12.14)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
      "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from tqdm->google-generativeai) (0.4.6)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.70.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.70.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai) (3.1.2)\n",
      "Requirement already satisfied: anyio in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.33->langchain) (2.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.4.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade langchain langchain-google-genai google-generativeai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e571c0d2",
   "metadata": {},
   "source": [
    "As you can see above, embedding for a sentance \"What is your refund policy\" is a list of size 768. Looking at the numbers in this list, doesn't give any intuitive understanding of what it is but just assume that these numbers are capturing the meaning of \"What is your refund policy\". If you are curious to know about embeddings, go to youtube and search \"codebasics word embeddings\" and you will find bunch of videos with simple, intuitive explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc80a28a",
   "metadata": {},
   "source": [
    "### Vector store using FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce98c326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 75 documents.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deeks\\AppData\\Local\\Temp\\ipykernel_22960\\1697016426.py:22: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  results = retriever.get_relevant_documents(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: I am not allowing to post doubt in the discord group\n",
      "response: Sure I can guide you\n",
      "\n",
      "Go to the 'click-here-to-ask-questions' section and verify yourself here by clicking on the checkmark.\n",
      "prompt: How can I get help if I have a doubt and need support?\n",
      "response: We have an active discord server where you can post your question and most of the time you will get an answer in a reasonable time frame.\n",
      "prompt: How can I contact the instructors for any doubts/support?\n",
      "response: We have created every lecture with a motive to explain everything in an easy-to-understand manner. While working on these lectures you could make mistakes in steps or have some doubts. You need to commit yourself to hold patience, make efforts & diagnose the errors yourself by googling in order to become truly job ready. For any questions, that Google cannot answer or if you hit a wall - we got you covered! You can join our active discord community. which is a dedicated platform to discuss & clear your doubts with fellow learners & mentors.\n",
      "prompt: I am getting this error, what is wrong?\n",
      "response: Go to ChatGPT (https://chat.openai.com/) and type this command\n",
      "\n",
      "Debug me this code ( copy paste your code) and also provide the error you are getting\n",
      "\n",
      "\n",
      "Why I am suggesting this way is once you learn it you will love it. \n",
      "And it will be SUPER USEFUL in your career when you get a data analyst jobs. Companies love folks who are capable of finding their answers on their own (Using Google, ChatGPT etc.)\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "\n",
    "loader = CSVLoader(file_path=\"codebasics_faqs.csv\", source_column=\"prompt\", encoding=\"ISO-8859-1\")\n",
    "data = loader.load()\n",
    "print(f\"Loaded {len(data)} documents.\")\n",
    "\n",
    "\n",
    "embedding_model = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/embedding-001\", \n",
    "    google_api_key=\"AIzaSyDz2Dt2LonIglhqMihyIJiAFeIgT82p9XA\"\n",
    ")\n",
    "\n",
    "vectordb = FAISS.from_documents(documents=data, embedding=embedding_model)\n",
    "\n",
    "\n",
    "retriever = vectordb.as_retriever(score_threshold=0.7)\n",
    "\n",
    "query = \"How do I use FAISS in LangChain?\"\n",
    "results = retriever.get_relevant_documents(query)\n",
    "\n",
    "for doc in results:\n",
    "    print(doc.page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "74bc8c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.9.0.post1-cp312-cp312-win_amd64.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\deeks\\anaconda3\\anaconda\\lib\\site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\deeks\\anaconda3\\anaconda\\lib\\site-packages (from faiss-cpu) (24.1)\n",
      "Downloading faiss_cpu-1.9.0.post1-cp312-cp312-win_amd64.whl (13.8 MB)\n",
      "   ---------------------------------------- 0.0/13.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/13.8 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/13.8 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/13.8 MB 1.4 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 0.8/13.8 MB 1.1 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 1.0/13.8 MB 1.2 MB/s eta 0:00:11\n",
      "   --- ------------------------------------ 1.3/13.8 MB 1.3 MB/s eta 0:00:10\n",
      "   ---- ----------------------------------- 1.6/13.8 MB 1.3 MB/s eta 0:00:10\n",
      "   ------ --------------------------------- 2.1/13.8 MB 1.4 MB/s eta 0:00:09\n",
      "   ------ --------------------------------- 2.4/13.8 MB 1.5 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 2.9/13.8 MB 1.5 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 3.4/13.8 MB 1.7 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 3.9/13.8 MB 1.7 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 4.5/13.8 MB 1.8 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 5.0/13.8 MB 1.9 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 5.8/13.8 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 6.6/13.8 MB 2.1 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 7.1/13.8 MB 2.2 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 7.9/13.8 MB 2.3 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 8.9/13.8 MB 2.4 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 9.7/13.8 MB 2.5 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 10.5/13.8 MB 2.5 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 11.0/13.8 MB 2.5 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 11.8/13.8 MB 2.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 12.6/13.8 MB 2.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 13.4/13.8 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 13.8/13.8 MB 2.7 MB/s eta 0:00:00\n",
      "Installing collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.9.0.post1\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1a503f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement faiss-gpu (from versions: none)\n",
      "ERROR: No matching distribution found for faiss-gpu\n"
     ]
    }
   ],
   "source": [
    "pip install faiss-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d5fef5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpuNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Using cached faiss_cpu-1.9.0.post1-cp312-cp312-win_amd64.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\deeks\\anaconda3\\lib\\site-packages (from faiss-cpu) (24.1)\n",
      "Using cached faiss_cpu-1.9.0.post1-cp312-cp312-win_amd64.whl (13.8 MB)\n",
      "Installing collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.9.0.post1\n"
     ]
    }
   ],
   "source": [
    "pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfd58f6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='07be2ac6-ada2-447f-98f3-44210d68d728', metadata={'source': 'Do you provide any job assistance?', 'row': 11}, page_content='prompt: Do you provide any job assistance?\\nresponse: Yes, We help you with resume and interview preparation along with that we help you in building online credibility, and based on requirements we refer candidates to potential recruiters.'),\n",
       " Document(id='bc9ed30c-7460-4978-bff8-b4c6d10dcb0b', metadata={'source': 'Will this bootcamp guarantee me a job?', 'row': 15}, page_content='prompt: Will this bootcamp guarantee me a job?\\nresponse: The courses included in this bootcamp are done by 9000+ learners and many of them have secured a job which gives us ample confidence that you will be able to get a job. However, we want to be honest and do not want to make any impractical promises! Our guarantee is to prepare you for the job market by teaching the most relevant skills, knowledge & timeless principles good enough to fetch the job.'),\n",
       " Document(id='c0ce2337-e4ce-4f6c-a39c-91d90d06d0c1', metadata={'source': 'Will this course guarantee me a job?', 'row': 33}, page_content='prompt: Will this course guarantee me a job?\\nresponse: We created a much lighter version of this course on YouTube available for free (click this link) and many people gave us feedback that they were able to fetch jobs (see testimonials). Now this paid course is at least 5x better than the YouTube course which gives us ample confidence that you will be able to get a job. However, we want to be honest and do not want to make any impractical promises! Our guarantee is to prepare you for the job market by teaching the most relevant skills, knowledge & timeless principles good enough to fetch the job.'),\n",
       " Document(id='b1f5e646-4af6-4320-bb9d-dd90f1259c9c', metadata={'source': 'Do you provide any virtual internship?', 'row': 14}, page_content='prompt: Do you provide any virtual internship?\\nresponse: Yes')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdocs = retriever.get_relevant_documents(\"how about job placement support?\")\n",
    "rdocs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf6b257",
   "metadata": {},
   "source": [
    "As you can see above, the retriever that was created using FAISS and hugging face embedding is now capable of pulling relavant documents from our original CSV file knowledge store. This is very powerful and it will help us further in our project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bee857",
   "metadata": {},
   "source": [
    "##### Embeddings can be created using GooglePalm too. Also for vector database you can use chromadb as well as shown below. During our experimentation, we found hugging face embeddings and FAISS to be more appropriate for our use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a93d079d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# google_palm_embeddings = GooglePalmEmbeddings(google_api_key=api_key)\n",
    "\n",
    "# from langchain.vectorstores import Chroma\n",
    "# vectordb = Chroma.from_documents(data,\n",
    "#                            embedding=google_palm_embeddings,\n",
    "#                            persist_directory='./chromadb')\n",
    "# vectordb.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f3d927",
   "metadata": {},
   "source": [
    "### Create RetrievalQA chain along with prompt template "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ad83d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "#  Define prompt template\n",
    "prompt_template = \"\"\"Given the following context and a question, generate an answer based on this context only.\n",
    "In the answer, try to provide as much text as possible from the \"response\" section in the source document without making many changes.\n",
    "If the answer is not found in the context, kindly state \"I don't know.\" Don't try to make up an answer.\n",
    "\n",
    "CONTEXT: {context}\n",
    "\n",
    "QUESTION: {question}\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "chain_type_kwargs = {\"prompt\": PROMPT}\n",
    "\n",
    "#  Initialize LLM (Choose one: Gemini or OpenAI GPT)\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", google_api_key=\"AIzaSyDz2Dt2LonIglhqMihyIJiAFeIgT82p9XA\")\n",
    "\n",
    "#  Ensure retriever is defined\n",
    "retriever = vectordb.as_retriever()\n",
    "\n",
    "#  Create the RetrievalQA chain\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs=chain_type_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152a4cf8",
   "metadata": {},
   "source": [
    "### We are all set  Let's ask some questions now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90166e8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Do you provide job assistance and also do you provide job gurantee?',\n",
       " 'result': \"I don't know.\",\n",
       " 'source_documents': [Document(id='07be2ac6-ada2-447f-98f3-44210d68d728', metadata={'source': 'Do you provide any job assistance?', 'row': 11}, page_content='prompt: Do you provide any job assistance?\\nresponse: Yes, We help you with resume and interview preparation along with that we help you in building online credibility, and based on requirements we refer candidates to potential recruiters.'),\n",
       "  Document(id='bc9ed30c-7460-4978-bff8-b4c6d10dcb0b', metadata={'source': 'Will this bootcamp guarantee me a job?', 'row': 15}, page_content='prompt: Will this bootcamp guarantee me a job?\\nresponse: The courses included in this bootcamp are done by 9000+ learners and many of them have secured a job which gives us ample confidence that you will be able to get a job. However, we want to be honest and do not want to make any impractical promises! Our guarantee is to prepare you for the job market by teaching the most relevant skills, knowledge & timeless principles good enough to fetch the job.'),\n",
       "  Document(id='c0ce2337-e4ce-4f6c-a39c-91d90d06d0c1', metadata={'source': 'Will this course guarantee me a job?', 'row': 33}, page_content='prompt: Will this course guarantee me a job?\\nresponse: We created a much lighter version of this course on YouTube available for free (click this link) and many people gave us feedback that they were able to fetch jobs (see testimonials). Now this paid course is at least 5x better than the YouTube course which gives us ample confidence that you will be able to get a job. However, we want to be honest and do not want to make any impractical promises! Our guarantee is to prepare you for the job market by teaching the most relevant skills, knowledge & timeless principles good enough to fetch the job.'),\n",
       "  Document(id='b1f5e646-4af6-4320-bb9d-dd90f1259c9c', metadata={'source': 'Do you provide any virtual internship?', 'row': 14}, page_content='prompt: Do you provide any virtual internship?\\nresponse: Yes')]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain('Do you provide job assistance and also do you provide job gurantee?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a4e3e4",
   "metadata": {},
   "source": [
    "**As you can see above, the answer of question comes from two different FAQs within our csv file and it is able to pull those questions and merge them nicely**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82dce73e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Do you guys provide internship and also do you offer EMI payments?',\n",
       " 'result': \"I don't know.\",\n",
       " 'source_documents': [Document(id='864d855d-9b99-433b-8c46-64b110cff07f', metadata={'source': 'Do we have an EMI option?', 'row': 13}, page_content='prompt: Do we have an EMI option?\\nresponse: No'),\n",
       "  Document(id='b1f5e646-4af6-4320-bb9d-dd90f1259c9c', metadata={'source': 'Do you provide any virtual internship?', 'row': 14}, page_content='prompt: Do you provide any virtual internship?\\nresponse: Yes'),\n",
       "  Document(id='07be2ac6-ada2-447f-98f3-44210d68d728', metadata={'source': 'Do you provide any job assistance?', 'row': 11}, page_content='prompt: Do you provide any job assistance?\\nresponse: Yes, We help you with resume and interview preparation along with that we help you in building online credibility, and based on requirements we refer candidates to potential recruiters.'),\n",
       "  Document(id='4b391727-e3bb-4c86-ba85-6c12524e3b60', metadata={'source': 'Can I add this course to my resume?', 'row': 19}, page_content='prompt: Can I add this course to my resume?\\nresponse: Yes. Absolutely you can mention the AtliQ Hardware project experience in your resume with the relevant skills that you will learn from this course')]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain(\"Do you guys provide internship and also do you offer EMI payments?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48970302",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'do you have javascript course?',\n",
       " 'result': \"I don't know. The provided context does not mention anything about JavaScript courses.\",\n",
       " 'source_documents': [Document(id='919df603-6243-41df-a54f-229037f08fc3', metadata={'source': '\\nI don\\x92t have a laptop, can I take this course?', 'row': 25}, page_content='prompt: I don\\x92t have a laptop, can I take this course?\\nresponse: We recommend learning by doing and therefore you need to have a laptop or a PC (at least 4 GB ram).'),\n",
       "  Document(id='a5eb5228-3c39-4252-a19f-5e7f00245411', metadata={'source': 'Can I add this course to my resume? If Yes, how?', 'row': 34}, page_content='prompt: Can I add this course to my resume? If Yes, how?\\nresponse: Absolutely, we have a section in this course explaining how you can add the learnings from this course in your resume that will appeal to the hiring team.'),\n",
       "  Document(id='b6a6226a-9198-410b-9ad0-9d37f1d30a03', metadata={'source': 'Why should I trust Codebasics?', 'row': 1}, page_content='prompt: Why should I trust Codebasics?\\nresponse: Till now 9000 + learners have benefitted from the quality of our courses. You can check the review section and also we have attached their LinkedIn profiles so that you can connect with them and ask directly.'),\n",
       "  Document(id='c0ce2337-e4ce-4f6c-a39c-91d90d06d0c1', metadata={'source': 'Will this course guarantee me a job?', 'row': 33}, page_content='prompt: Will this course guarantee me a job?\\nresponse: We created a much lighter version of this course on YouTube available for free (click this link) and many people gave us feedback that they were able to fetch jobs (see testimonials). Now this paid course is at least 5x better than the YouTube course which gives us ample confidence that you will be able to get a job. However, we want to be honest and do not want to make any impractical promises! Our guarantee is to prepare you for the job market by teaching the most relevant skills, knowledge & timeless principles good enough to fetch the job.')]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain(\"do you have javascript course?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c17dc6c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Do you have plans to launch blockchain course in future?',\n",
       " 'result': \"I don't know.\",\n",
       " 'source_documents': [Document(id='a5eb5228-3c39-4252-a19f-5e7f00245411', metadata={'source': 'Can I add this course to my resume? If Yes, how?', 'row': 34}, page_content='prompt: Can I add this course to my resume? If Yes, how?\\nresponse: Absolutely, we have a section in this course explaining how you can add the learnings from this course in your resume that will appeal to the hiring team.'),\n",
       "  Document(id='4b391727-e3bb-4c86-ba85-6c12524e3b60', metadata={'source': 'Can I add this course to my resume?', 'row': 19}, page_content='prompt: Can I add this course to my resume?\\nresponse: Yes. Absolutely you can mention the AtliQ Hardware project experience in your resume with the relevant skills that you will learn from this course'),\n",
       "  Document(id='b1f5e646-4af6-4320-bb9d-dd90f1259c9c', metadata={'source': 'Do you provide any virtual internship?', 'row': 14}, page_content='prompt: Do you provide any virtual internship?\\nresponse: Yes'),\n",
       "  Document(id='939020e0-8328-45fa-bda9-c6d8e424cf9e', metadata={'source': 'What is the duration of this bootcamp? How long will it last?', 'row': 8}, page_content='prompt: What is the duration of this bootcamp? How long will it last?\\nresponse: You can complete all courses in 3 months if you dedicate 2-3 hours per day.')]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain(\"Do you have plans to launch blockchain course in future?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c35c2c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'should I learn power bi or tableau?',\n",
       " 'result': \"I don't know.\",\n",
       " 'source_documents': [Document(id='35dab637-5818-4c9b-a550-fd7d99cd1cb5', metadata={'source': '\\nPower BI or Tableau which one is better?', 'row': 29}, page_content='prompt: Power BI or Tableau which one is better?\\nresponse: This is a contextual question. If you are talking about a pure visualization tool Tableau is slightly better. Data connectors, modeling and transformation features are available in both. However, factually speaking Power BI is cheaper and offers tighter integration with the Microsoft environment. Since most companies use excel & Microsoft tools they start with Power BI or move towards Power BI for seamless integration with other Microsoft tools (called as Power platform). This makes the job openings grow at a much higher rate on Power BI and Power Platform. Also, Power BI has been leading the Gartner\\x92s magic quadrant in BI for the last few years as the industry leader.'),\n",
       "  Document(id='0dbd9e8d-fa01-409b-87ad-ad9cec0d83d8', metadata={'source': 'I use tableau, can I take this course?', 'row': 28}, page_content='prompt: I use tableau, can I take this course?\\nresponse: Yes, you will still benefit from the concepts outside the tools that are discussed in this course such as business context, problem solving, project management tools, etc.'),\n",
       "  Document(id='df3cf4d9-3822-4f65-83ba-686d3ef82a9d', metadata={'source': 'What is different in this course from thousands of other Power BI courses available online?', 'row': 36}, page_content='prompt: What is different in this course from thousands of other Power BI courses available online?\\nresponse: Most of the courses available on the internet teach you how to build x & y without any business context and do not prepare you for the real business world. This course is rather an experience in which you will learn how to use Power BI & other non-technical skills to solve a real-life business problem using analytics. Here you focus on solving a business problem and in that process learn how Power BI can be used as a tool. This is how you will do the work when you start working as a data analyst/ Business analyst/ Power BI developer in the industry. This course will prepare you for not just fetching the job but, shine in it & grow further.'),\n",
       "  Document(id='8038c5f8-28ed-449b-819b-9ec79207623b', metadata={'source': 'I already know basic Power BI, what benefit do I get by taking this course?', 'row': 37}, page_content='prompt: I already know basic Power BI, what benefit do I get by taking this course?\\nresponse: This course is taught through a true end-to-end project in a Consumer goods company involving all the steps mimicking the real business environment, so you will learn how to execute end-to-end projects Power BI projects successfully along with the business fundamentals. You will learn a lot of extra things such as Project management tools, effective communication techniques & organizational nuances.')]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain(\"should I learn power bi or tableau?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a054c5ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': \"I've a MAC computer. Can I use powerbi on it?\",\n",
       " 'result': 'Power BI desktop works only in Windows OS. However, you can use a virtual machine to install and work with Power BI in other Operating systems.',\n",
       " 'source_documents': [Document(id='c6890116-f235-4a9d-b75e-470c408095e4', metadata={'source': 'How can I use PowerBI on my Mac system?', 'row': 44}, page_content='prompt: How can I use PowerBI on my Mac system?\\nresponse: Hi\\n\\nYou can use VirtualBox to create a virtual machine and install Windows on it. This will allow you to run Power BI and Excel on your Mac.\\n\\nIf you\\'re not familiar with setting up a virtual machine, there are many resources available on YouTube that can guide you through the process. Simply search for \"installing virtual machines\" and you\\'ll find plenty of helpful videos.\\n\\nBest of luck with your studies!'),\n",
       "  Document(id='ef424ddc-920b-42dd-b6ac-34fd0bb966bf', metadata={'source': 'Does Power BI work in Mac OS/Ubuntu?', 'row': 31}, page_content='prompt: Does Power BI work in Mac OS/Ubuntu?\\nresponse: Power BI desktop works only in Windows OS. Please look into the system requirements section on this page. However, you can use a virtual machine to install and work with Power BI in other Operating systems.'),\n",
       "  Document(id='24c377f7-b8be-4e20-adfc-b36333aada80', metadata={'source': 'How do I enable Power Pivot before using it for the first time ?', 'row': 74}, page_content='prompt: How do I enable Power Pivot before using it for the first time ?\\nresponse: Follow the process in the link : \\n\\nhttps://drive.google.com/file/d/1-mO-v52h-YTY1s-v30liBJPu6Yj4OUxb/view?usp=share_link'),\n",
       "  Document(id='919df603-6243-41df-a54f-229037f08fc3', metadata={'source': '\\nI don\\x92t have a laptop, can I take this course?', 'row': 25}, page_content='prompt: I don\\x92t have a laptop, can I take this course?\\nresponse: We recommend learning by doing and therefore you need to have a laptop or a PC (at least 4 GB ram).')]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain(\"I've a MAC computer. Can I use powerbi on it?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89fa5d10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': \"I don't see power pivot. how can I enable it?\",\n",
       " 'result': \"I don't know.\",\n",
       " 'source_documents': [Document(id='24c377f7-b8be-4e20-adfc-b36333aada80', metadata={'source': 'How do I enable Power Pivot before using it for the first time ?', 'row': 74}, page_content='prompt: How do I enable Power Pivot before using it for the first time ?\\nresponse: Follow the process in the link : \\n\\nhttps://drive.google.com/file/d/1-mO-v52h-YTY1s-v30liBJPu6Yj4OUxb/view?usp=share_link'),\n",
       "  Document(id='13b42a91-cd8f-4c54-930a-401431936bad', metadata={'source': 'How to install power pivot if its not available in system?', 'row': 38}, page_content='prompt: How to install power pivot if its not available in system?\\nresponse: Follow this thread for instructions - https://support.microsoft.com/en-us/office/start-the-power-pivot-add-in-for-excel-a891a66d-36e3-43fc-81e8-fc4798f39ea8\\nIf it doesn\\'t show in the ribbon then go to \"insert\" tab. You will be able to see pivot table option there.'),\n",
       "  Document(id='57ae51f7-9d0e-4c89-baff-ce7cebc7843a', metadata={'source': 'How do I update source in power query ?', 'row': 73}, page_content='prompt: How do I update source in power query ?\\nresponse: Follow the discord link : \\n\\n https://discord.com/channels/1090613684163850280/1114113976616353832/1114121330925768754'),\n",
       "  Document(id='cc00f2be-e2ac-4e5f-9cf7-cb05eb220d8a', metadata={'source': 'whenever I am loading fact_sales_monthly table in Power query not all columns are visible', 'row': 60}, page_content=\"prompt: whenever I am loading fact_sales_monthly table in Power query not all columns are visible\\nresponse: Have you taken SQL course in Codebasics first ?\\nIf yes, delete the existing databases in MySQL workbench and re-import again. After this go to 'Home' tab in Power Query and click on Refresh button and see if it works any.\\n\\nCheck this reference too: https://discordapp.com/channels/1090613684163850280/1111180401478746163/1111190746507251732\")]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain(\"I don't see power pivot. how can I enable it?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6539e58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What is the price of your machine learning course?',\n",
       " 'result': \"I don't know.\",\n",
       " 'source_documents': [Document(id='51d3a91f-f68b-4bb3-bea2-62864e61fa67', metadata={'source': 'I\\x92m not sure if this course is good enough for me to invest some money. What can I do?', 'row': 20}, page_content='prompt: I\\x92m not sure if this course is good enough for me to invest some money. What can I do?\\nresponse: Don\\x92t worry. Many videos in this course are free so watch them to get an idea of the quality of teaching. Dhaval Patel (the course instructor) runs a popular data science YouTube channel called Codebasics. On that, you can watch his videos and read comments to get an idea of his teaching style'),\n",
       "  Document(id='0a6d7c9d-08b8-46cf-a0e8-29433892a7e2', metadata={'source': 'Is there any prerequisite for taking this course?', 'row': 35}, page_content='prompt: Is there any prerequisite for taking this course?\\nresponse: The only prerequisite is that you need to have a functional laptop with at least 4GB ram, internet connection and a thrill to learn data analysis.'),\n",
       "  Document(id='c0ce2337-e4ce-4f6c-a39c-91d90d06d0c1', metadata={'source': 'Will this course guarantee me a job?', 'row': 33}, page_content='prompt: Will this course guarantee me a job?\\nresponse: We created a much lighter version of this course on YouTube available for free (click this link) and many people gave us feedback that they were able to fetch jobs (see testimonials). Now this paid course is at least 5x better than the YouTube course which gives us ample confidence that you will be able to get a job. However, we want to be honest and do not want to make any impractical promises! Our guarantee is to prepare you for the job market by teaching the most relevant skills, knowledge & timeless principles good enough to fetch the job.'),\n",
       "  Document(id='625cfa59-90f9-475d-b077-f7c162a9cadb', metadata={'source': 'Once purchased, is this course available for lifetime access?', 'row': 22}, page_content='prompt: Once purchased, is this course available for lifetime access?\\nresponse: Yes')]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain(\"What is the price of your machine learning course?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
